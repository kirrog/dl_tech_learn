# Отчёт "Лабораторная №2"

**Работу выполнили студенты 1 курса магистратуры (группа M4130) ИТМО:** Меньщиков Михаил
, Лютиков Богдан, Рогачёв Кирилл.

**Дисциплина:** Технологии программирования и обучения глубоких сетей

**ИТМО 2023**

## Содержание:

[1 Постановка задания (ответственный: Кирилл, Богдан)](#1-постановка-задания-ответственный-кирилл-богдан)

[2 Решение (ответственный: Михаил)](#-2-решение-ответственный-михаил)

[2.1 Описание](#_u2hphc60i0ic)

[2.2 Структура кода](#_7e3gf6yg9c9m)

[2.3 Выводы](#_m91mjtol24oj)

[3 Заключение (ответственный: Кирилл, Богдан)](#_gzjf31x596w7)

[Список использованных источников](#_pzqzcu57tmmk)

[Приложение А](#_ha1lzi3a35tm)

##

## **1 Постановка задания** (ответственный: Кирилл, Богдан)


**Цель:** Научиться работать с предобученными моделями и на основе предобученных эмбеддингов строить новые модели.

**Задание:**

1. Скачать датасет[1](https://www.kaggle.com/datasets/rajatrc1705/youtube-videos-dataset) или[2](https://coin-dataset.github.io/) (можно взять youtube8m).
2. Отфильтровать датасет.
3. Скачать необходимые видео.
4. Получить эмбеддинги, указанные в варианте и построить нейронную сеть на основе них.
5. Оценить качество модели на тесте.
6. Сделать отчёт и выложить исходный код на GitHub.

**Выбранный вариант:** 6 (получение frame-level признаков; применение аугментации cutout)

## <a name="#_aks2ghux0ozg"></a> **2 Решение** (ответственный: Михаил)

## <a name="#_u2hphc60i0ic"></a> **2.1 Описание**

В качестве тренировочного датасета был использован "Youtube Videos Dataset" [1]. Датасет представляет из себя список URL-ссылок на видеоролики с платформы YouTube. Каждой ссылке поставлена в соответствие одна из четырёх категорий видео: "travel", "art\_music", "food", "history". Объём датасета составляет ~3400 ссылок, но в данном решении была использована только часть из них.

С помощью командной утилиты "yt-dlp" [2] c YouTube было скачано следующее количество видеороликов каждой из категорий: "travel" – 340; "art\_music" – 247; "food" – 389; "history" – 275 (суммарно 1251 ролик). В результате анализа было выявлено, что в категории "art\_music" присутствуют видеоролики продолжительностью более 30 минут, у которых при этом в качестве видеоряда фигурирует одно статическое изображение. Такие "статические" видео могут повлиять в худшую сторону на обобщающую способность результирующей модели-классификатора. В случае использования таких роликов модель будет обучена на одинаковых кадрах из видеоряда и сможет предсказывать верную метку только для увиденных многократно однотипных шаблонов. В связи с этим подобные ролики были исключены из скаченного набора. Также некоторые ролики присутствовали сразу в нескольких категориях. Такие ролики также были исключены из скаченного набора. Таким образом распределение роликов по категориям после фильтрации имеет следующий вид: "travel" – 337; "art\_music" – 173; "food" – 385; "history" – 275 (суммарно 1170 роликов). На Рисунке 1 представлено распределение количества кадров в видеороликах для каждой из категорий.

![](RackMultipart20240115-1-c6kaog_html_9874db6d10179f0d.png)﻿

**Рисунок 1.** Распределение количества кадров в роликах по категориям

На основании Рисунка 1 максимальное, среднее и минимальное количество кадров на видеоролик из соответствующей категории имеет следующие значения:

- "travel": max – 235975; mean – 24659.436; min – 487.
- "art\_music": max – 55139; mean – 12942.243; min – 2410.
- "food": max – 135495; mean – 24965.158; min – 0.
- "history": max – 4280403; mean – 87389.287; min – 0.

После успешного скачивания и фильтрации роликов из них были извлечены кадры (frames) для формирования обучающей выборки. Извлечение кадров для каждого ролика выполнялось по следующему алгоритму:

1. Из видеоряда удаляются первые 200 и последние 500 кадров. Это связано с тем, что в них присутствует однотипная заставка и титры к ролику, которые могут повлиять в худшую сторону на обобщающую способность модели. Количество кадров на удаление было выбрано эмпирическим методом: исходя из анализа имеющегося набора видеороликов.
2. Из урезанного видеоряда извлекаются кадры с шагом (в количество кадров), в зависимости от категории обрабатываемого ролика: "travel" – 500, "history" – 1000; "food" – 500; "art\_music" – 100. Длина шага для каждой категории выбиралась эмпирическим методом: исходя из анализа продолжительности видеороликов и при учёте необходимости сбалансированности классов в тренировочной выборке.

В результате применения вышеописанного алгоритма извлечения кадров была получена тренировочная выборка со следующим количеством экземпляров на категорию: "travel" – 16309, "history" – 23975; "food" – 18874; "art\_music" – 21268 (суммарно 80426 кадров).

Далее для каждого кадра был получен его эмбеддинг. Для этого использовался бэкбон Mask2Former-модели для задачи сегментации [3]. В качестве бэкбона у данной модели выступает Swim-Transformer [4]. Размерность эмбеддинга составляет 1536. Перед получением эмбеддинга кадра он был аугументирован при помощи Cutout-алгоритма [5]. Для сокращения временных затрат на получение эмбеддингов была арендована видеокарта RTX3090 [6]. Было потрачено следующее время на вычисления: "travel" – 20 минут, "history" – 27 минут; "food" – 21 минута; "art\_music" – 24 минуты (суммарно ~1.5 часа).

В качестве модели многоклассовой классификации был использован CatBoostClassifier из библиотеки catboost [7]. Были указаны следующие значения гиперпарметров для обучения модели: eval\_metric – "Accuracy"; iterations – 1000; learning\_rate – 0.5; early\_stopping\_rounds – 20; random\_seed – 63; loss\_function – "MultiClass"; task\_type – "GPU"; devices – "0:1". Исходная выборка была разбита на train/test-части в соотношении 80/20. На Рисунке 2 и Рисунке 3 представлено изменение общего качества модели и качества предсказания конкретной категории в процессе обучения на тестовой выборке соответственно.

![](RackMultipart20240115-1-c6kaog_html_5d80d6aeb1cf3876.png)

**Рисунок 2.** Изменение качества модели в процессе обучения

![](RackMultipart20240115-1-c6kaog_html_d8171520d45ce264.png)

**Рисунок 3.** Изменение качества предсказания конкретной категории в процессе обучения модели

В результате обучения catboost-модели для задачи предсказания метки класса (одного из четырёх) по кадру видео-ролика было получено качество, представленное на Рисунке 4.

![](RackMultipart20240115-1-c6kaog_html_f5bd919a624e0dd6.jpg)

**Рисунок 4.** Качество модели классификации кадров видеоролика

Относительно высокая ошибка предсказания для категории "travel" связана с тем, что в роликах про путешествия люди нередко снимают приём/приготовление пищи и модель относит кадры с подобным содержанием к категории "food".

Для отнесения целого видеоролика к конкретной категории используется следующий алгоритм:

1. Из видеоряда удаляются первые 5% и последние 10% кадров. Процент кадров на удаление был выбран эмпирическим методом: исходя из анализа имеющегося набора видеороликов.
2. Из урезанного видеоряда извлекаются кадры с шагом в количество FPS данного видеоролика для сокращения временных затрат на предсказание категории.
3. Для извлечённых кадров выполняется классификация на принадлежность к одной из четырёх категорий с помощью обученной catboost-модели.
4. В качестве категории всего видеоролика выбирается та, которая встречается чаще всего для его кадров.

Для тестирования вышеописанного алгоритма из каждой категории было выбрано по 5 видеороликов. В результате тестирования алгоритм предсказал верную категорию для всех 20 роликов. Зависимость между временем предсказания метки видеоролика и количеством кадров представлено на Рисунке 5. Тестирование проводилось на машине со следующими характеристиками:

- ОС – Linux Mint 21.2 Victoria 64-bit;
- CPU – Intel® Core™ i5-9400F CPU @ 2.90GHz × 6;
- RAM – 23.4 GB;
- GPU – NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2;
- HDD - WD Blue 1 TB.

![](RackMultipart20240115-1-c6kaog_html_9fab338bf56920f7.png)

**Рисунок 5.** Зависимость времени предсказания категории видеоролика от количества содержащихся кадров.

Из Рисунка 5 видно, что для классификации видеоролика с 10 тыс. кадров требуется ~1.5 минуты; для 30 тыс. кадров ~3.5 минуты; для 60 тыс. кадров ~7.5 минут.

## **2.2 Структура кода**

Иерархия каталога с программной реализацией проведённого эксперимента выглядит следующим образом:

- /datasets
  - cleaned\_data.csv – исходный датасет с возможными роликами на скачивание и их категориями.
  - video\_label\_predictions.csv – качества конвейера предсказания категории для видеороликов.
  - video\_dataset.csv – используемый набор видеороликов для обучения модели классификации кадров.
- /debug
  - video\_classificator\_debug.ipynb – пример реализации конвейера по предсказанию категории для видеоролика.
- /metrics – директория с метриками и лог-файлами, полученных в процессе обучения используемых моделей.
- download\_videos.py – выгрузка видеороликов с платформы YouTube.
- fix\_videos.py – изменение кодировки у выгруженных видеороликов.
- get\_frames\_embeddings.py – получение векторных представлений для кадров видеоролика.
- train\_catboost\_model.py – обучение модели классифкации кадров видеоролика.
- video\_classificator\_test.py – тестирование алгоритма предсказания категории для видеоролика.

## **2.3 Выводы**

Таким образом, была обучена модель классификации кадров и реализован конвейер классификации видеороликов на основании его frame-level признаков. Для модели классификации кадров видеоролика на 4 категории значение метрики Accuracy равно 0.86 при объёме тестовой выборки 16086 примеров (~4 тыс. примеров на категорию). Для выбранных 20 видеороликов в качестве тестового набора (5 роликов на категорию) конвейер для всех из них верно предсказал метку.

**Недостатки.** (1) В полученном решении используется несколько эмпирически полученных констант: число кадров, вырезаемых в начале и конце видео. Такое решение не является оптимальным, так как для некоторых видеороликов мы теряем часть информации о его содержании, которые может потребоваться для предсказания верной категории. (2) Используемое тренировочное множество для обучения модели классификации кадров содержало примеры с шумом: часть кадров из роликов с категорией "travel" должны принадлежать к категории "food"-кадров. Из-за этого модель имеет недостаточно высокую метрику распознавания кадров с категорией "travel". (3) Для того, чтобы предсказать категорию видеоролика, необходимо выполнить классификацию метки для каждого его кадра. Из-за этого время получения итогового результата составляет более 1 минуты при количестве кадров более 10000 (линейная зависимость), что является не достаточно хорошим показателем.

**Возможные улучшения.** (1) Разработать модель машинного обучения, позволяющую динамически для каждого видеоролика определять: какое количество кадров в начале и в конце последовательности кадров можно отбросить. (2) Расширить тренировочное множество за счёт использования большего количества видеороликов; провести более глубокий анализ используемых кадров и удалить примеры с шумом; подобрать оптимальные способы аугментации данных (3) Для предсказания категории видеоролика выполнять классификацию его кадров в параллельном режиме.

## **3 Заключение** (ответственный: Кирилл, Богдан)

В результате выполнения данной лабораторной был получен практический опыт обучения моделей машинного обучения с применением эмбединнгов от предобученных нейросетевых моделей. Была решена задача классификации видеороликов на 4 категории по их frame-level признакам. Была использована cutout-аугментация в процессе обучения модели-классификатора.

## **Список использованных источников**

1. Youtube Videos Dataset (~3400 videos) [Электронный ресурс] // kaggle.com. – 2023. – URL: [https://www.kaggle.com/datasets/rajatrc1705/youtube-videos-dataset](https://www.kaggle.com/datasets/rajatrc1705/youtube-videos-dataset) (дата обращения: 28.12.23)
2. yt-dlp [Электронный ресурс] // github.com. – 2023. – URL: [https://github.com/yt-dlp/yt-dlp](https://github.com/yt-dlp/yt-dlp) (дата обращения: 28.12.23)
3. mask2former-swin-large-cityscapes-semantic [Электронный ресурс] // huggingface.co. – 2023. – URL: [https://huggingface.co/facebook/mask2former-swin-large-cityscapes-semantic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-semantic) (дата обращения: 28.12.23)
4. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows [Электронный ресурс] // github.com. – 2023. – URL: [https://arxiv.org/pdf/2103.14030.pdf](https://arxiv.org/pdf/2103.14030.pdf) (дата обращения: 28.12.23)
5. Cutout [Электронный ресурс] // paperswithcode.com. – 2023. – URL: [https://paperswithcode.com/method/cutout](https://paperswithcode.com/method/cutout) (дата обращения: 28.12.23)
6. Immers Cloud [Электронный ресурс] // immers.cloud. – 2023. – URL: [https://immers.cloud/](https://immers.cloud/) (дата обращения: 28.12.23)
7. CatBoost [Электронный ресурс] // [catboost.ai](https://catboost.ai/) – 2023. – URL: [https://catboost.ai/](https://catboost.ai/) (дата обращения: 28.12.23)

## **Приложение А**

**Описание SwimTransformer-архитектуры**

TODO
