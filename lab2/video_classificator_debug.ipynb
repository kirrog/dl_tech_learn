{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import cv2\n",
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import os\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from math import floor, ceil\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краткое описание алгоритма:\n",
    "* получаем путь до видео\n",
    "* извлекаем фреймы\n",
    "* обрезаем последовательность фреймов\n",
    "* классифицируем каждый фрейм\n",
    "* получаем распределение предсказанных меток\n",
    "* выводим результат классификации\n",
    "<br>===<br>\n",
    "* тестируем данный алгоритм на тестовом наборе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['travel','art_music','food','history']\n",
    "ID2LABEL = {i:v for i, v in enumerate(LABELS)}\n",
    "LABEL2ID = {v:i for i, v in enumerate(LABELS)}\n",
    "ROOT_DIR = './'\n",
    "EMBEDDER_PATH = ROOT_DIR + 'pretrained_model/facebook-m2f_swin_large'\n",
    "EMBEDDER_PROCESSOR_PATH = ROOT_DIR + 'pretrained_model/facebook-m2f_swin_large'\n",
    "FRAME_CLASSIFIER_PATH = ROOT_DIR + 'frame_classifier_model'\n",
    "START_PROP_OFFSET = 5\n",
    "END_PROP_OFFSET = 10\n",
    "EMBEDDINGS_SIZE = 1536\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedderProcessor:\n",
    "    def __init__(self, processor_path=EMBEDDER_PROCESSOR_PATH):\n",
    "        self.transform_part1 = A.Compose([A.augmentations.dropout.coarse_dropout.CoarseDropout(\n",
    "            max_height=16, max_width=16, max_holes=16)])\n",
    "        self.transform_part2 = AutoImageProcessor.from_pretrained(processor_path)\n",
    "\n",
    "    def transform(self, frame):\n",
    "        image_tensor1 = self.transform_part1(image=frame, return_tensors=\"pt\")['image']\n",
    "        image_tensor2 = self.transform_part2(image_tensor1, return_tensors=\"pt\")['pixel_values']\n",
    "        return image_tensor2\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self,embedder_path=EMBEDDER_PATH):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        #\n",
    "        m2f = Mask2FormerForUniversalSegmentation.from_pretrained(embedder_path)\n",
    "        m2f.requires_grad_(False)\n",
    "        self.bb_features = 1536\n",
    "\n",
    "        # M2F backbone\n",
    "        self.embeddings = m2f.model.pixel_level_module.encoder.embeddings\n",
    "        self.encoder = m2f.model.pixel_level_module.encoder.encoder\n",
    "        self.layernorm = nn.LayerNorm(self.bb_features)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_output, input_dimensions = self.embeddings(x)\n",
    "        encoder_outputs = self.encoder(embedding_output, input_dimensions)\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "\n",
    "        sequence_output = self.layernorm(sequence_output)\n",
    "        pooled_output = self.pooler(sequence_output.transpose(1, 2))\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "def classify_video(video_path, emb_model, emb_processor, fcls_model):\n",
    "    print(\"Loading video...\", end='')\n",
    "    s_time = time()\n",
    "    video_object = load_video(video_path)\n",
    "    e_time = time()\n",
    "    print(round(e_time - s_time, 3), \"sec\")\n",
    "\n",
    "\n",
    "    print(\"Cuting head and tail...\", end='')\n",
    "    s_time = time()\n",
    "    selected_frame_ids = reduce_video(video_object)\n",
    "    label_freqs = {label: 0 for label in LABELS}\n",
    "    frames_amount = len(selected_frame_ids)\n",
    "    e_time = time()\n",
    "    print(round(e_time - s_time, 3), \"sec\")\n",
    "\n",
    "    print(\"Predicting labels for selected frames...\")\n",
    "    process = tqdm(selected_frame_ids)\n",
    "    for frame_id in process:\n",
    "        ret, frame = get_frame_by_id(video_object, frame_id)\n",
    "        frame_emb = get_frame_embedding(frame, emb_model, emb_processor)\n",
    "        pred_label = classify_frame(frame_emb, fcls_model)\n",
    "        label_freqs[pred_label] += 1\n",
    "        process.set_description_str(pred_label)\n",
    "\n",
    "    print(\"Calculating labels proportion...\", end='')\n",
    "    s_time = time()\n",
    "    label_probs = {k: round(v / frames_amount, 3) for k,v in label_freqs.items()}\n",
    "    frequent_label = sorted(label_probs.items(), key=lambda v: v[1], reverse=True)[0]\n",
    "    e_time = time()\n",
    "    print(round(e_time - s_time, 3), \"sec\")\n",
    "\n",
    "    print(\"Done!\")\n",
    "    return frequent_label[0], label_probs\n",
    "\n",
    "def load_video(video_path):\n",
    "    return cv2.VideoCapture(video_path)\n",
    "\n",
    "def get_frame_by_id(video_object, frame_id):\n",
    "    video_object.set(1, frame_id)\n",
    "    return video_object.read()\n",
    "\n",
    "def get_frame_embedding(frame, emb_model, emb_processor):\n",
    "    transformed_frame = emb_processor.transform(frame)\n",
    "    with torch.no_grad():\n",
    "        output = emb_model(transformed_frame.to(DEVICE))\n",
    "    frame_embedding = output.view(-1, EMBEDDINGS_SIZE).detach().cpu().numpy().tolist()[0]\n",
    "\n",
    "    return frame_embedding\n",
    "\n",
    "\n",
    "def classify_frame(frame_emb, fcls_model):\n",
    "    output = fcls_model.predict(frame_emb)\n",
    "    return ID2LABEL[output[0]]\n",
    "\n",
    "def init_models(emb_model_path, emb_processor_path, fcls_model_path):\n",
    "    emb_model = Embedder(emb_model_path).to(DEVICE)\n",
    "    emb_model.eval()\n",
    "\n",
    "    emb_processor = EmbedderProcessor(emb_processor_path)\n",
    "    \n",
    "    fcls_model = CatBoostClassifier()\n",
    "    fcls_model.load_model(fcls_model_path)\n",
    "\n",
    "    return emb_model, emb_processor, fcls_model\n",
    "\n",
    "def reduce_video(video_object):\n",
    "    frames_amount = int(video_object.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames_id = list(range(frames_amount))\n",
    "    fps = ceil(video_object.get(cv2.CAP_PROP_FPS))\n",
    "    print(f\"[before reduce: {frames_amount}]...\",end='')\n",
    "\n",
    "    filtered_frames = frames_id[::fps]\n",
    "    print(f\"[after fps filter: {len(filtered_frames)}]...\",end='')\n",
    "\n",
    "    head_offset = (len(filtered_frames) * START_PROP_OFFSET) // 100\n",
    "    tail_offset = (len(filtered_frames) * END_PROP_OFFSET) // 100\n",
    "\n",
    "    return filtered_frames[head_offset:-tail_offset] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model, emb_processor, fcls_model = init_models(EMBEDDER_PATH, EMBEDDER_PROCESSOR_PATH, FRAME_CLASSIFIER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = '/home/dzigen/Desktop/ITMO/sem1/DLtech/dl_tech_learn/lab2/fixed_videos/food/_1g735GyVlU.mp4'\n",
    "print(classify_video(VIDEO_FILE, emb_model, emb_processor, fcls_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEOS_PER_LABEL = 10\n",
    "VIDEO_DATASET_PATH = './videos_dataset.csv'\n",
    "VIDEO_DIR = './fixed_videos/'\n",
    "LABELS = os.listdir(VIDEO_DIR)\n",
    "PREDICTIONS_FILE = 'video_label_predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = pd.read_csv(VIDEO_DATASET_PATH,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model, emb_processor, fcls_model = init_models(EMBEDDER_PATH, EMBEDDER_PROCESSOR_PATH, \n",
    "                                                   FRAME_CLASSIFIER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "pred_distributions = []\n",
    "used_links = []\n",
    "elapsed_time = []\n",
    "for label in LABELS:\n",
    "    selected_links = videos_df[videos_df['category'] == label]['links'].to_list()[:VIDEOS_PER_LABEL]\n",
    "    references += [label] * VIDEOS_PER_LABEL\n",
    "    used_links += selected_links\n",
    "\n",
    "    for i, link in enumerate(selected_links):\n",
    "        print(f\"{label} | {link} | {i} / {len(selected_links)}\")\n",
    "        s_time = time()\n",
    "        path_to_cur_video = f\"{VIDEO_DIR}{label}/{link}\"\n",
    "        pred_label, label_distr = classify_video(path_to_cur_video, emb_model, emb_processor, fcls_model)\n",
    "        e_time = time()\n",
    "        elapsed_time.append(round(e_time-s_time, 3))\n",
    "        predictions.append(pred_label)\n",
    "        pred_distributions.append(label_distr)\n",
    "\n",
    "test_df = pd.DataFrame((used_links, references, predictions, pred_distributions, elapsed_time), columns=['links','refs','preds','distrs','elapsed_time (sec)'])\n",
    "test_df.to_csv(PREDICTIONS_FILE, sep=';',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(map(lambda v: LABEL2ID[v], references))\n",
    "y_pred = list(map(lambda v: LABEL2ID[v], predictions))\n",
    "print(classification_report(y_true, y_pred, target_names=LABELS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
