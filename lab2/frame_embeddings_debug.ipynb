{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from random import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2F_MODEL_NAME = \"./pretrained_model/facebook-m2f_swin_large\"\n",
    "FRAMES_DIR = '/home/dzigen/Desktop/ITMO/sem1/DLtech/lab2/frames'\n",
    "LABEL_NAMES = os.listdir(FRAMES_DIR)\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Mask2Former fine-tuned on Cityscapes semantic segmentation\n",
    "#M2F_MODEL_NAME = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n",
    "#m2f_processor = AutoImageProcessor.from_pretrained(M2F_MODEL_NAME)\n",
    "#m2f = Mask2FormerForUniversalSegmentation.from_pretrained(M2F_MODEL_NAME)\n",
    "#m2f_processor.save_pretrained(\"./pretrained_model/facebook-m2f_swin_large\")\n",
    "#m2f.save_pretrained(\"./pretrained_model/facebook-m2f_swin_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M2Fencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(M2Fencoder, self).__init__()\n",
    "        \n",
    "        #\n",
    "        m2f = Mask2FormerForUniversalSegmentation.from_pretrained(M2F_MODEL_NAME)\n",
    "        m2f.requires_grad_(False)\n",
    "        self.bb_features = 1536\n",
    "\n",
    "        # M2F backbone\n",
    "        self.embeddings = m2f.model.pixel_level_module.encoder.embeddings\n",
    "        self.encoder = m2f.model.pixel_level_module.encoder.encoder\n",
    "        self.layernorm = nn.LayerNorm(self.bb_features)\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_output, input_dimensions = self.embeddings(x)\n",
    "        encoder_outputs = self.encoder(embedding_output, input_dimensions)\n",
    "        sequence_output = encoder_outputs.last_hidden_state\n",
    "\n",
    "        sequence_output = self.layernorm(sequence_output)\n",
    "        pooled_output = self.pooler(sequence_output.transpose(1, 2))\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "def show_pil_imgs(images):\n",
    "  num_images = len(images)\n",
    "  fig, axs = plt.subplots(1, num_images)\n",
    "  if num_images == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "  for i in range(num_images):\n",
    "      axs[i].imshow(images[i])\n",
    "      axs[i].axis('off')\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images_names, images_dir, transform) -> None:\n",
    "\n",
    "        self.images_dir = images_dir\n",
    "        self.image_names = images_names\n",
    "        self.transform_part1 = A.Compose([A.augmentations.dropout.coarse_dropout.CoarseDropout(\n",
    "            max_height=16, max_width=16, max_holes=16)])\n",
    "        self.transform_part2 = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_filepath = f\"{self.images_dir}/{self.image_names[index]}\"\n",
    "\n",
    "        image = cv2.imread(image_filepath)\n",
    "        #print(image)\n",
    "\n",
    "        image_tensor1 = self.transform_part1(image=image, return_tensors=\"pt\")['image']\n",
    "        image_tensor2 = self.transform_part2(image_tensor1, return_tensors=\"pt\")['pixel_values']\n",
    "\n",
    "        return image_tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_model = M2Fencoder().to(DEVICE)\n",
    "m2f_processor = AutoImageProcessor.from_pretrained(M2F_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "for frames_dir_name in LABEL_NAMES:\n",
    "    cur_dir_path = f\"{FRAMES_DIR}/{frames_dir_name}\"\n",
    "    frame_names = os.listdir(cur_dir_path)\n",
    "\n",
    "    dataset[frames_dir_name] =  ImageDataset(frame_names, cur_dir_path, m2f_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "travel 16309\n",
      "history 23975\n",
      "food 18874\n",
      "art_music 21268\n"
     ]
    }
   ],
   "source": [
    "for frames_dir_name in LABEL_NAMES:\n",
    "    print(frames_dir_name, len(dataset[frames_dir_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "travel: 100%|██████████| 10/10 [00:03<00:00,  3.20it/s]\n",
      "history: 100%|██████████| 10/10 [00:02<00:00,  3.73it/s]\n",
      "food: 100%|██████████| 10/10 [00:02<00:00,  3.84it/s]\n",
      "art_music: 100%|██████████| 10/10 [00:03<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "tmp_df = []\n",
    "for label_name in LABEL_NAMES:\n",
    "    #len(dataset[label_name])\n",
    "    process = tqdm(range(10))\n",
    "    for i in process:\n",
    "        process.set_description_str(label_name)\n",
    "        output = m2f_model(dataset[label_name][i].to(DEVICE))\n",
    "        image_embedding = output.view(-1, 1536).detach().cpu().numpy().tolist()\n",
    "        tmp_df.append((image_embedding[0], label_name))\n",
    "    df = pd.DataFrame(tmp_df,columns=[\"embeddings\",\"labels\"])\n",
    "    df.to_csv(\"frames_embeddings.csv\", index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE1 = './frames_embeddings.csv'\n",
    "FILE2 = './frames_embeddings4.csv'\n",
    "\n",
    "df1 = pd.read_csv(FILE1, sep=';')\n",
    "df2 = pd.read_csv(FILE2, sep=';')\n",
    "\n",
    "df_union = pd.concat([df1, df2]).reset_index(drop=True)\n",
    "df_union.to_csv(\"frames_embeddings_4labels.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from time import time\n",
    "\n",
    "FILE = './frames_embeddings.csv'\n",
    "X_COL = 'embeddings'\n",
    "Y_COL = 'labels'\n",
    "\n",
    "print(\"read file...\",end='')\n",
    "s_time = time()\n",
    "df = pd.read_csv(FILE,sep=';')\n",
    "e_time = time()\n",
    "print(f\"{round(e_time-s_time,3)} sec\")\n",
    "\n",
    "print(\"formate file...\",end='')\n",
    "s_time=time()\n",
    "df[X_COL] = df[X_COL].apply(lambda v: literal_eval(v))\n",
    "e_time=time()\n",
    "print(f\"{round(e_time-s_time,3)} sec\")\n",
    "\n",
    "tmp_df = []\n",
    "embeddings_size = len(df[X_COL][0])\n",
    "for i in range(df.shape[0]):\n",
    "    tmp_df.append(df[X_COL] + df[Y_COL])\n",
    "\n",
    "new_df = pd.DataFrame(tmp_df,columns=[f\"x{i}\" for i in range(embeddings_size)] + ['labels'])\n",
    "new_df.to_csv(\"formated_frames_embeddings_4labels.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
