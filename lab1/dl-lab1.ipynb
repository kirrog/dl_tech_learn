{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":46697,"sourceType":"datasetVersion","datasetId":31559},{"sourceId":6903787,"sourceType":"datasetVersion","datasetId":3965307}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n\n# try:\n#     import os, sys\n\n#     kernel_path = os.path.abspath(os.path.join('..'))\n#     sys.path.append(kernel_path)\n#     from kernels.window_process.window_process import WindowProcess, WindowProcessReverse\n\n\n\nclass SwinTransformer(nn.Module):\n    r\"\"\" Swin Transformer\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n          https://arxiv.org/pdf/2103.14030\n\n    Args:\n        img_size (int | tuple(int)): Input image size. Default 224\n        patch_size (int | tuple(int)): Patch size. Default: 4\n        in_chans (int): Number of input image channels. Default: 3\n        num_classes (int): Number of classes for classification head. Default: 1000\n        embed_dim (int): Patch embedding dimension. Default: 96\n        depths (tuple(int)): Depth of each Swin Transformer layer.\n        num_heads (tuple(int)): Number of attention heads in different layers.\n        window_size (int): Window size. Default: 7\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n        drop_rate (float): Dropout rate. Default: 0\n        attn_drop_rate (float): Attention dropout rate. Default: 0\n        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n                 use_checkpoint=False, fused_window_process=False, **kwargs):\n        super().__init__()\n\n        self.num_classes = num_classes\n        self.num_layers = len(depths)\n        self.embed_dim = embed_dim\n        self.ape = ape\n        self.patch_norm = patch_norm\n        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n        self.mlp_ratio = mlp_ratio\n\n        # split image into non-overlapping patches\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n            norm_layer=norm_layer if self.patch_norm else None)\n        num_patches = self.patch_embed.num_patches\n        patches_resolution = self.patch_embed.patches_resolution\n        self.patches_resolution = patches_resolution\n\n        # absolute position embedding\n        if self.ape:\n            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n            trunc_normal_(self.absolute_pos_embed, std=.02)\n\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # stochastic depth\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\n        # build layers\n        self.layers = nn.ModuleList()\n        for i_layer in range(self.num_layers):\n            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n                                                 patches_resolution[1] // (2 ** i_layer)),\n                               depth=depths[i_layer],\n                               num_heads=num_heads[i_layer],\n                               window_size=window_size,\n                               mlp_ratio=self.mlp_ratio,\n                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n                               drop=drop_rate, attn_drop=attn_drop_rate,\n                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n                               norm_layer=norm_layer,\n                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n                               use_checkpoint=use_checkpoint,\n                               fused_window_process=fused_window_process)\n            self.layers.append(layer)\n\n        self.norm = norm_layer(self.num_features)\n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'absolute_pos_embed'}\n\n    @torch.jit.ignore\n    def no_weight_decay_keywords(self):\n        return {'relative_position_bias_table'}\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        if self.ape:\n            x = x + self.absolute_pos_embed\n        x = self.pos_drop(x)\n\n        for layer in self.layers:\n            x = layer(x)\n\n        x = self.norm(x)  # B L C\n        x = self.avgpool(x.transpose(1, 2))  # B C 1\n        x = torch.flatten(x, 1)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.head(x)\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    r\"\"\" Image to Patch Embedding\n\n    Args:\n        img_size (int): Image size.  Default: 224.\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        patches_resolution = [img_size[0] //\n                              patch_size[0], img_size[1] // patch_size[1]]\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.patches_resolution = patches_resolution\n        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim,\n                              kernel_size=patch_size, stride=patch_size)\n        if norm_layer is not None:\n            self.norm = norm_layer(embed_dim)\n        else:\n            self.norm = None\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n        if self.norm is not None:\n            x = self.norm(x)\n        return x\n\n\nclass PatchMerging(nn.Module):\n    r\"\"\" Patch Merging Layer.\n\n    Args:\n        input_resolution (tuple[int]): Resolution of input feature.\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n\n    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n        self.norm = norm_layer(4 * dim)\n\n    def forward(self, x):\n        \"\"\"\n        x: B, H*W, C\n        \"\"\"\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\n        x = x.view(B, H, W, C)\n\n        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x\n\n\nclass WindowAttention(nn.Module):\n    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n    \"\"\"\n\n    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size  # Wh, Ww\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # define a parameter table of relative position bias\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(self.window_size[0])\n        coords_w = torch.arange(self.window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - \\\n            coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(\n            1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += self.window_size[0] - \\\n            1  # shift to start from 0\n        relative_coords[:, :, 1] += self.window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        self.register_buffer(\"relative_position_index\",\n                             relative_position_index)\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n        trunc_normal_(self.relative_position_bias_table, std=.02)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, mask=None):\n        \"\"\"\n        Args:\n            x: input features with shape of (num_windows*B, N, C)\n            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n        \"\"\"\n        B_, N, C = x.shape\n        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C //\n                                  self.num_heads).permute(2, 0, 3, 1, 4)\n        # make torchscript happy (cannot use tensor as tuple)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n        relative_position_bias = relative_position_bias.permute(\n            2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        attn = attn + relative_position_bias.unsqueeze(0)\n\n        if mask is not None:\n            nW = mask.shape[0]\n            attn = attn.view(B_ // nW, nW, self.num_heads, N,\n                             N) + mask.unsqueeze(1).unsqueeze(0)\n            attn = attn.view(-1, self.num_heads, N, N)\n            attn = self.softmax(attn)\n        else:\n            attn = self.softmax(attn)\n\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\ndef window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\ndef window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"\n    B = int(windows.shape[0] / (H * W / window_size / window_size))\n    x = windows.view(B, H // window_size, W // window_size,window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n    return x\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwinTransformerBlock(nn.Module):\n    r\"\"\" Swin Transformer Block.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resulotion.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 fused_window_process=False):\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.num_heads = num_heads\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.mlp_ratio = mlp_ratio\n        if min(self.input_resolution) <= self.window_size:\n            # if window size is larger than input resolution, we don't partition windows\n            self.shift_size = 0\n            self.window_size = min(self.input_resolution)\n        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\n        self.norm1 = norm_layer(dim)\n        self.attn = WindowAttention(\n            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if self.shift_size > 0:\n            # calculate attention mask for SW-MSA\n            H, W = self.input_resolution\n            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n            h_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            w_slices = (slice(0, -self.window_size),\n                        slice(-self.window_size, -self.shift_size),\n                        slice(-self.shift_size, None))\n            cnt = 0\n            for h in h_slices:\n                for w in w_slices:\n                    img_mask[:, h, w, :] = cnt\n                    cnt += 1\n\n            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n        else:\n            attn_mask = None\n\n        self.register_buffer(\"attn_mask\", attn_mask)\n        self.fused_window_process = fused_window_process\n\n    def forward(self, x):\n        H, W = self.input_resolution\n        B, L, C = x.shape\n        assert L == H * W, \"input feature has wrong size\"\n\n        shortcut = x\n        x = self.norm1(x)\n        x = x.view(B, H, W, C)\n\n        # cyclic shift\n        if self.shift_size > 0:\n            if not self.fused_window_process:\n                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n                # partition windows\n                x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n            else:\n                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)\n        else:\n            shifted_x = x\n            # partition windows\n            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n\n        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n\n        # W-MSA/SW-MSA\n        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\n        # merge windows\n        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n\n        # reverse cyclic shift\n        if self.shift_size > 0:\n            if not self.fused_window_process:\n                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n            else:\n                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)\n        else:\n            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n            x = shifted_x\n        x = x.view(B, H * W, C)\n        x = shortcut + self.drop_path(x)\n\n        # FFN\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\n        return x\n\n\nclass BasicLayer(nn.Module):\n    \"\"\" A basic Swin Transformer layer for one stage.\n\n    Args:\n        dim (int): Number of input channels.\n        input_resolution (tuple[int]): Input resolution.\n        depth (int): Number of blocks.\n        num_heads (int): Number of attention heads.\n        window_size (int): Local window size.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n        drop (float, optional): Dropout rate. Default: 0.0\n        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False\n    \"\"\"\n\n    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n                 fused_window_process=False):\n\n        super().__init__()\n        self.dim = dim\n        self.input_resolution = input_resolution\n        self.depth = depth\n        self.use_checkpoint = use_checkpoint\n\n        # build blocks\n        self.blocks = nn.ModuleList([\n            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n                                 num_heads=num_heads, window_size=window_size,\n                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n                                 mlp_ratio=mlp_ratio,\n                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n                                 drop=drop, attn_drop=attn_drop,\n                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n                                 norm_layer=norm_layer,\n                                 fused_window_process=fused_window_process)\n            for i in range(depth)])\n\n        # patch merging layer\n        if downsample is not None:\n            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:07:25.886114Z","iopub.execute_input":"2023-11-29T10:07:25.886368Z","iopub.status.idle":"2023-11-29T10:07:30.816005Z","shell.execute_reply.started":"2023-11-29T10:07:25.886345Z","shell.execute_reply":"2023-11-29T10:07:30.814871Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torchvision.transforms import v2 as T\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.io import loadmat\nfrom PIL import Image\nfrom pathlib import Path\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-11-29T15:25:03.420520Z","iopub.execute_input":"2023-11-29T15:25:03.420787Z","iopub.status.idle":"2023-11-29T15:25:07.438977Z","shell.execute_reply.started":"2023-11-29T15:25:03.420762Z","shell.execute_reply":"2023-11-29T15:25:07.438010Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n  warnings.warn(_BETA_TRANSFORMS_WARNING)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import re, os\nimage_labels = []\nre_color = re.compile(r'(?<=\\$\\$).*?(?=\\$\\$)')\nfor path, dirs, files in os.walk('/kaggle/input/dvm-confirmed-fronts/confirmed_fronts_fixnames'):\n    if len(files) > 0:\n        for file in files:\n            image_labels.append({'path': str(Path(path, file)),\n                                 'color': re_color.findall(file)[2]})","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:43:42.425343Z","iopub.execute_input":"2023-11-29T12:43:42.426115Z","iopub.status.idle":"2023-11-29T12:44:54.991748Z","shell.execute_reply.started":"2023-11-29T12:43:42.426078Z","shell.execute_reply":"2023-11-29T12:44:54.990907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_anno = pd.read_csv('/kaggle/input/stanford-car-dataset-by-classes-folder/anno_train.csv', names=['file_name','bx1', 'by1', 'bx2', 'by2', 'class'])","metadata":{"execution":{"iopub.status.busy":"2023-11-29T09:17:11.992102Z","iopub.execute_input":"2023-11-29T09:17:11.992595Z","iopub.status.idle":"2023-11-29T09:17:12.009293Z","shell.execute_reply.started":"2023-11-29T09:17:11.992558Z","shell.execute_reply":"2023-11-29T09:17:12.008298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re, os\ndef create_data(path_anno, path_from):\n    image_labels = []\n    for path, dirs, files in os.walk(path_from):\n        if len(files) > 0:\n            for file in files:\n                image_labels.append({'path': str(Path(path, file))})\n    train_anno = pd.read_csv(path_anno, names=['file_name','bx1', 'by1', 'bx2', 'by2', 'class'])            \n    paths = pd.DataFrame(image_labels)\n    paths['file_name'] = paths['path'].apply(lambda x: Path(x).parts[-1])\n    return train_anno.merge(paths, on='file_name')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T15:25:07.440823Z","iopub.execute_input":"2023-11-29T15:25:07.441330Z","iopub.status.idle":"2023-11-29T15:25:07.450261Z","shell.execute_reply.started":"2023-11-29T15:25:07.441297Z","shell.execute_reply":"2023-11-29T15:25:07.449040Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data = create_data('/kaggle/input/stanford-car-dataset-by-classes-folder/anno_train.csv',\n                        '/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/train')\ntest_data = create_data('/kaggle/input/stanford-car-dataset-by-classes-folder/anno_test.csv',\n                        '/kaggle/input/stanford-car-dataset-by-classes-folder/car_data/car_data/test')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T15:25:07.451778Z","iopub.execute_input":"2023-11-29T15:25:07.452123Z","iopub.status.idle":"2023-11-29T15:25:12.174078Z","shell.execute_reply.started":"2023-11-29T15:25:07.452092Z","shell.execute_reply":"2023-11-29T15:25:12.173302Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"crop_classes = train_data['class'].value_counts().where(lambda x: x > 44).dropna().index","metadata":{"execution":{"iopub.status.busy":"2023-11-29T15:25:12.175634Z","iopub.execute_input":"2023-11-29T15:25:12.176001Z","iopub.status.idle":"2023-11-29T15:25:12.183972Z","shell.execute_reply.started":"2023-11-29T15:25:12.175956Z","shell.execute_reply":"2023-11-29T15:25:12.183010Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"crop_classes.values.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-29T15:25:37.204572Z","iopub.execute_input":"2023-11-29T15:25:37.204945Z","iopub.status.idle":"2023-11-29T15:25:37.210753Z","shell.execute_reply.started":"2023-11-29T15:25:37.204916Z","shell.execute_reply":"2023-11-29T15:25:37.209911Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(40,)"},"metadata":{}}]},{"cell_type":"code","source":"crop_train_data = train_data[train_data['class'].isin(crop_classes)]\ncrop_test_data = test_data[test_data['class'].isin(crop_classes)]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:08:01.295930Z","iopub.execute_input":"2023-11-29T10:08:01.296649Z","iopub.status.idle":"2023-11-29T10:08:01.303927Z","shell.execute_reply.started":"2023-11-29T10:08:01.296618Z","shell.execute_reply":"2023-11-29T10:08:01.302488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crop_train_data.to_csv('/kaggle/working/train_data.csv', index=False)\ncrop_test_data.to_csv('/kaggle/working/test_data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:08:01.818524Z","iopub.execute_input":"2023-11-29T10:08:01.818821Z","iopub.status.idle":"2023-11-29T10:08:01.858055Z","shell.execute_reply.started":"2023-11-29T10:08:01.818797Z","shell.execute_reply":"2023-11-29T10:08:01.857076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame(image_labels)\ndata","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:44:54.993219Z","iopub.execute_input":"2023-11-29T12:44:54.993508Z","iopub.status.idle":"2023-11-29T12:44:55.057569Z","shell.execute_reply.started":"2023-11-29T12:44:54.993483Z","shell.execute_reply":"2023-11-29T12:44:55.056524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data[data['color'] != 'Unlisted']","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:44:55.058797Z","iopub.execute_input":"2023-11-29T12:44:55.059544Z","iopub.status.idle":"2023-11-29T12:44:55.075348Z","shell.execute_reply.started":"2023-11-29T12:44:55.059509Z","shell.execute_reply":"2023-11-29T12:44:55.074573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def under_sampling(data, t = 1000, n = 1000):\n    data = data[data['color'].isin(data['color'].value_counts().index[data['color'].value_counts() >= t].values)]\n    dataframe = pd.concat([data[data['color'] == color].sample(n, replace=True) for color in data['color'].unique()])\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:44:55.077649Z","iopub.execute_input":"2023-11-29T12:44:55.078031Z","iopub.status.idle":"2023-11-29T12:44:55.084188Z","shell.execute_reply.started":"2023-11-29T12:44:55.077998Z","shell.execute_reply":"2023-11-29T12:44:55.083301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_data = under_sampling(data, 500, 1000)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:44:55.085277Z","iopub.execute_input":"2023-11-29T12:44:55.085946Z","iopub.status.idle":"2023-11-29T12:44:55.236198Z","shell.execute_reply.started":"2023-11-29T12:44:55.085913Z","shell.execute_reply":"2023-11-29T12:44:55.235472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_data['color'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:47:08.492803Z","iopub.execute_input":"2023-11-28T15:47:08.493607Z","iopub.status.idle":"2023-11-28T15:47:08.503609Z","shell.execute_reply.started":"2023-11-28T15:47:08.493571Z","shell.execute_reply":"2023-11-28T15:47:08.502689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampled_data.to_csv('/kaggle/working/data.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:24.326641Z","iopub.execute_input":"2023-11-29T12:45:24.327349Z","iopub.status.idle":"2023-11-29T12:45:24.421839Z","shell.execute_reply.started":"2023-11-29T12:45:24.327313Z","shell.execute_reply":"2023-11-29T12:45:24.420999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\ndata = pd.read_csv('/kaggle/working/data.csv')\nX = data.drop(columns=['color'])\nY = data.loc[:, 'color']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:26.269434Z","iopub.execute_input":"2023-11-29T12:45:26.269812Z","iopub.status.idle":"2023-11-29T12:45:26.299309Z","shell.execute_reply.started":"2023-11-29T12:45:26.269779Z","shell.execute_reply":"2023-11-29T12:45:26.298576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport pandas as pd\n\nlabel_to_idx = {val: idx for idx, val in enumerate(Y.unique())}\n\nclass ImageDataset(Dataset):\n    def __init__(self, image_paths: pd.DataFrame, image_color: pd.Series, transform):\n        print(\"Reading Image Dataset...\")\n        self.image_paths = image_paths\n        self.classes = image_color\n        self.label_to_idx = label_to_idx\n        self.transform = transform\n        print(\"Image Dataset instance created!\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_path = self.image_paths['path'].iloc[index]\n        image = Image.open(img_path).convert('RGB')\n        # image = Image.open(img_path).convert('HSV')\n        label = self.classes.iloc[index]\n\n        image_tensor = self.transform(image)\n        image.close()\n        return image_tensor, self.label_to_idx[label]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:30.645939Z","iopub.execute_input":"2023-11-29T12:45:30.646648Z","iopub.status.idle":"2023-11-29T12:45:30.655473Z","shell.execute_reply.started":"2023-11-29T12:45:30.646616Z","shell.execute_reply":"2023-11-29T12:45:30.654531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = crop_train_data['path']\ny_train = crop_train_data['class']\nX_test = crop_test_data['path']\ny_test = crop_test_data['class']","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:08:09.858561Z","iopub.execute_input":"2023-11-29T10:08:09.859476Z","iopub.status.idle":"2023-11-29T10:08:09.865143Z","shell.execute_reply.started":"2023-11-29T10:08:09.859432Z","shell.execute_reply":"2023-11-29T10:08:09.863863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.iloc[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:08:11.123509Z","iopub.execute_input":"2023-11-29T10:08:11.123913Z","iopub.status.idle":"2023-11-29T10:08:11.131222Z","shell.execute_reply.started":"2023-11-29T10:08:11.123879Z","shell.execute_reply":"2023-11-29T10:08:11.130299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport pandas as pd\n\nlabel_to_idx = {val: idx for idx, val in enumerate(y_train.unique())}\n\nclass ImageDataset(Dataset):\n    def __init__(self, image_paths: pd.DataFrame, image_color: pd.Series, transform):\n        print(\"Reading Image Dataset...\")\n        self.image_paths = image_paths\n        self.classes = image_color\n        self.label_to_idx = label_to_idx\n        self.transform = transform\n        print(\"Image Dataset instance created!\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, index):\n        img_path = self.image_paths.iloc[index]\n        image = Image.open(img_path).convert('RGB')\n        # image = Image.open(img_path).convert('HSV')\n        label = self.classes.iloc[index]\n\n        image_tensor = self.transform(image)\n        image.close()\n        return image_tensor, self.label_to_idx[label]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T10:08:19.299601Z","iopub.execute_input":"2023-11-29T10:08:19.299968Z","iopub.status.idle":"2023-11-29T10:08:19.308773Z","shell.execute_reply.started":"2023-11-29T10:08:19.299937Z","shell.execute_reply":"2023-11-29T10:08:19.307739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = T.Compose([\n    T.Resize((224, 224)),\n    T.RandomHorizontalFlip(),\n    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n    T.ToTensor(),\n#     T.ToDtype(torch.float32, scale=True)\n])\n\ntest_trainsform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n#     T.ToDtype(torch.float32, scale=True)\n])","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:39.013779Z","iopub.execute_input":"2023-11-29T12:45:39.014530Z","iopub.status.idle":"2023-11-29T12:45:39.020962Z","shell.execute_reply.started":"2023-11-29T12:45:39.014494Z","shell.execute_reply":"2023-11-29T12:45:39.019946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageDataset('cars_train_annos.mat', 'CarDatasets/cars_train/cars_train/', train_transform)\ntest_dataset = ImageDataset('cars_test_annos_withlabels_eval.mat', 'CarDatasets/cars_test/cars_test/', test_trainsform)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:47:09.247599Z","iopub.execute_input":"2023-11-28T15:47:09.247896Z","iopub.status.idle":"2023-11-28T15:47:09.256225Z","shell.execute_reply.started":"2023-11-28T15:47:09.247871Z","shell.execute_reply":"2023-11-28T15:47:09.255396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageDataset(image_paths=X_train,\n                                    image_color=y_train,\n                                    transform=train_transform)\ntest_dataset = ImageDataset(image_paths=X_test,\n                                   image_color=y_test,\n                                   transform=test_trainsform)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:43.613738Z","iopub.execute_input":"2023-11-29T12:45:43.614102Z","iopub.status.idle":"2023-11-29T12:45:43.619524Z","shell.execute_reply.started":"2023-11-29T12:45:43.614071Z","shell.execute_reply":"2023-11-29T12:45:43.618647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\n\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              pin_memory=True,\n                              drop_last=True)\n\ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=batch_size,\n                             shuffle=False,\n                             pin_memory=True,\n                             drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:45.101149Z","iopub.execute_input":"2023-11-29T12:45:45.101508Z","iopub.status.idle":"2023-11-29T12:45:45.107194Z","shell.execute_reply.started":"2023-11-29T12:45:45.101477Z","shell.execute_reply":"2023-11-29T12:45:45.106078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from SwinTranformer import SwinTransformer","metadata":{"execution":{"iopub.status.busy":"2023-11-28T15:47:09.283620Z","iopub.execute_input":"2023-11-28T15:47:09.283991Z","iopub.status.idle":"2023-11-28T15:47:09.882038Z","shell.execute_reply.started":"2023-11-28T15:47:09.283955Z","shell.execute_reply":"2023-11-28T15:47:09.880795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"swtf = SwinTransformer()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:47.412520Z","iopub.execute_input":"2023-11-29T12:45:47.413256Z","iopub.status.idle":"2023-11-29T12:45:47.826224Z","shell.execute_reply.started":"2023-11-29T12:45:47.413219Z","shell.execute_reply":"2023-11-29T12:45:47.825341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:47.828086Z","iopub.execute_input":"2023-11-29T12:45:47.828602Z","iopub.status.idle":"2023-11-29T12:45:47.833322Z","shell.execute_reply.started":"2023-11-29T12:45:47.828563Z","shell.execute_reply":"2023-11-29T12:45:47.832328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = swtf.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:48.152122Z","iopub.execute_input":"2023-11-29T12:45:48.152457Z","iopub.status.idle":"2023-11-29T12:45:48.193108Z","shell.execute_reply.started":"2023-11-29T12:45:48.152428Z","shell.execute_reply":"2023-11-29T12:45:48.192304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nlearning_rate = 1e-4\n\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.Adam(swtf.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:52.314409Z","iopub.execute_input":"2023-11-29T12:45:52.315137Z","iopub.status.idle":"2023-11-29T12:45:52.321445Z","shell.execute_reply.started":"2023-11-29T12:45:52.315101Z","shell.execute_reply":"2023-11-29T12:45:52.320439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nfrom tqdm import tqdm\n\ndef run_epoch(phase, dataloader):\n  if phase == 'train':\n      model.train()\n  else:\n      model.eval()\n\n  running_loss = 0.0\n  running_corrects = 0\n  y_test = []\n  y_pred = []\n  all_elems_count = 0\n  cur_tqdm = tqdm(dataloader)\n  for inputs, labels in cur_tqdm:\n      bz = inputs.shape[0]\n      all_elems_count += bz\n\n      inputs = inputs.to(device, non_blocking=True)\n      labels = labels.to(device, non_blocking=True)\n\n      outputs = model(inputs)\n      # if phase == 'train':\n      #   outputs, aux = model(inputs)\n      # else:\n      #    outputs = model(inputs)\n      \n      loss = criterion(outputs, labels)\n\n      if phase == 'train':\n          optimizer.zero_grad()\n          loss.backward()\n          optimizer.step()\n\n      _, preds = torch.max(outputs, 1)\n      y_test.extend(labels.detach().cpu().numpy())\n      y_pred.extend(preds.detach().cpu().numpy())\n      running_loss += loss.item() * bz\n      corrects_cnt = torch.sum(preds == labels.detach())\n      running_corrects += corrects_cnt\n      show_dict = {'Loss': f'{loss.item():.6f}',\n                    'Corrects': f'{corrects_cnt.item()}/{bz}',\n                    'Accuracy': f'{(corrects_cnt * 100 / bz).item():.3f}%'}\n      cur_tqdm.set_postfix(show_dict)\n\n  conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\n  print(\"Calculating metrics...\")\n  f05_macro = metrics.fbeta_score(y_test, y_pred, average=\"macro\", beta=0.5)\n  f1_macro = metrics.f1_score(y_test, y_pred, average=\"macro\")\n  epoch_loss = running_loss / all_elems_count\n  epoch_acc = running_corrects.float().item() / all_elems_count\n  return epoch_loss, epoch_acc, f05_macro, f1_macro, conf_matrix\n\ndef test_epoch(dataloader):\n    with torch.inference_mode():\n      return run_epoch('test', dataloader)\n\ndef train_epoch(dataloader):\n    return run_epoch('train', dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:53.285823Z","iopub.execute_input":"2023-11-29T12:45:53.286638Z","iopub.status.idle":"2023-11-29T12:45:53.298324Z","shell.execute_reply.started":"2023-11-29T12:45:53.286604Z","shell.execute_reply":"2023-11-29T12:45:53.297469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport math\nimport os\nfrom matplotlib import pyplot as plt\n\nlog_folder = 'logs'\nos.makedirs(log_folder, exist_ok=True)\n\nsaved_epoch_losses = {}\nsaved_epoch_accuracies = {}\nsaved_epoch_f1_macros = {}\n\ndef train_model(dataloaders, num_epochs=5):\n  print(f\"Training model with params:\")\n  print(f\"Optim: {optimizer}\")\n  print(f\"Criterion: {criterion}\")\n\n  phases = ['train', 'test']\n  for phase in dataloaders:\n      if phase not in phases:\n          phases.append(phase)\n\n  saved_epoch_losses = {phase: [] for phase in phases}\n  saved_epoch_accuracies = {phase: [] for phase in phases}\n  saved_epoch_f1_macros = {phase: [] for phase in phases}\n\n  for epoch in range(1, num_epochs + 1):\n      start_time = time.time()\n\n      print(\"=\" * 100)\n      print(f'Epoch {epoch}/{num_epochs}')\n      print('-' * 10)\n\n      for phase in phases:\n          print(\"--- Cur phase:\", phase)\n          epoch_loss, epoch_acc, f05_macro, f1_macro, conf_matrix = \\\n              train_epoch(dataloaders[phase]) if phase == 'train' \\\n                  else test_epoch(dataloaders[phase])\n          saved_epoch_losses[phase].append(epoch_loss)\n          saved_epoch_accuracies[phase].append(epoch_acc)\n          saved_epoch_f1_macros[phase].append(f1_macro)\n          print(f'{phase} loss: {epoch_loss:.6f}, '\n                f'acc: {epoch_acc:.6f}, '\n                f'f05_macro: {f05_macro:.6f}, '\n                f'f1_macro: {f1_macro:.6f}')\n          print(\"Confusion matrix:\")\n          print(conf_matrix)\n\n      model.eval()\n      if epoch > 1:\n        plt.title(f'Losses during training. Epoch {epoch}/{num_epochs}.')\n        plt.plot(range(1, epoch + 1), saved_epoch_losses['train'], label='Train Loss')\n        plt.plot(range(1, epoch + 1), saved_epoch_losses['test'], label='Test Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel(criterion.__class__.__name__)\n        plt.legend(loc=\"upper left\")\n        plt.savefig(f'{log_folder}/loss_graph_epoch{epoch + 1}.png')\n        plt.show()\n        plt.close('all')\n\n        plt.title(f'Accuracies during training. Epoch {epoch}/{num_epochs}.')\n        plt.plot(range(1, epoch + 1), saved_epoch_accuracies['train'], label='Train Acc')\n        plt.plot(range(1, epoch + 1), saved_epoch_accuracies['test'], label='Test Acc')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend(loc=\"upper left\")\n        plt.savefig(f'{log_folder}/acc_graph_epoch{epoch + 1}.png')\n        plt.show()\n        plt.close('all')\n\n      end_time = time.time()\n      epoch_time = end_time - start_time\n      print(\"-\" * 10)\n      print(f\"Epoch Time: {math.floor(epoch_time // 60)}:{math.floor(epoch_time % 60):02d}\")\n\n  print(\"*** Training Completed ***\")\n\n  return saved_epoch_losses, saved_epoch_accuracies, saved_epoch_f1_macros","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:45:54.613505Z","iopub.execute_input":"2023-11-29T12:45:54.614371Z","iopub.status.idle":"2023-11-29T12:45:54.629700Z","shell.execute_reply.started":"2023-11-29T12:45:54.614334Z","shell.execute_reply":"2023-11-29T12:45:54.628832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 30\ndataloaders = {'train': train_dataloader, 'test': test_dataloader}\n\nsaved_epoch_losses, saved_epoch_accuracies, saved_epoch_f1_macros = train_model(dataloaders, num_epochs)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:46:12.026619Z","iopub.execute_input":"2023-11-29T12:46:12.027138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam_saved_epoch_losses, adam_saved_epoch_accuracies, adam_saved_epoch_f1_macros = saved_epoch_losses, saved_epoch_accuracies, saved_epoch_f1_macros","metadata":{"execution":{"iopub.status.busy":"2023-11-29T11:15:25.338159Z","iopub.execute_input":"2023-11-29T11:15:25.338887Z","iopub.status.idle":"2023-11-29T11:15:25.342914Z","shell.execute_reply.started":"2023-11-29T11:15:25.338849Z","shell.execute_reply":"2023-11-29T11:15:25.342070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Radam_saved_epoch_losses, Radam_saved_epoch_accuracies, Radam_saved_epoch_f1_macros= saved_epoch_losses, saved_epoch_accuracies, saved_epoch_f1_macros","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:17:10.435249Z","iopub.execute_input":"2023-11-29T12:17:10.435620Z","iopub.status.idle":"2023-11-29T12:17:10.440356Z","shell.execute_reply.started":"2023-11-29T12:17:10.435590Z","shell.execute_reply":"2023-11-29T12:17:10.439365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(adam_saved_epoch_losses['test'], color='blue', label='Adam')\nplt.plot(Radam_saved_epoch_losses['test'], color='orange', label='RAdam')\nplt.title('epoch_losses')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:35:12.641036Z","iopub.execute_input":"2023-11-29T12:35:12.641435Z","iopub.status.idle":"2023-11-29T12:35:12.867930Z","shell.execute_reply.started":"2023-11-29T12:35:12.641399Z","shell.execute_reply":"2023-11-29T12:35:12.867062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(adam_saved_epoch_accuracies['test'], color='blue', label='Adam')\nplt.plot(Radam_saved_epoch_accuracies['test'], color='orange', label='RAdam')\nplt.title('epoch_accuracies')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T12:35:30.598726Z","iopub.execute_input":"2023-11-29T12:35:30.599523Z","iopub.status.idle":"2023-11-29T12:35:30.890978Z","shell.execute_reply.started":"2023-11-29T12:35:30.599481Z","shell.execute_reply":"2023-11-29T12:35:30.890142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/model.pt')","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:51:45.779432Z","iopub.execute_input":"2023-11-28T16:51:45.780220Z","iopub.status.idle":"2023-11-28T16:51:45.946056Z","shell.execute_reply.started":"2023-11-28T16:51:45.780184Z","shell.execute_reply":"2023-11-28T16:51:45.945252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-11-28T16:57:51.449187Z","iopub.execute_input":"2023-11-28T16:57:51.449947Z","iopub.status.idle":"2023-11-28T16:57:51.453905Z","shell.execute_reply.started":"2023-11-28T16:57:51.449915Z","shell.execute_reply":"2023-11-28T16:57:51.452983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classification_report(y_t, y_pred, target_names=list(label_to_idx.keys()))","metadata":{"execution":{"iopub.status.busy":"2023-11-28T17:39:09.918749Z","iopub.execute_input":"2023-11-28T17:39:09.919363Z","iopub.status.idle":"2023-11-28T17:39:09.938998Z","shell.execute_reply.started":"2023-11-28T17:39:09.919331Z","shell.execute_reply":"2023-11-28T17:39:09.938154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"              precision    recall  f1-score   support\n\n         Red       0.94      0.73      0.83       206\n        Blue       0.75      0.86      0.80       202\n       Black       0.59      0.72      0.65       184\n       White       0.84      0.83      0.83       214\n       Beige       0.61      0.85      0.71       197\n      Silver       0.68      0.49      0.57       204\n        Grey       0.56      0.30      0.39       210\n      Yellow       0.88      0.94      0.91       178\n       Green       0.80      0.67      0.73       198\n      Orange       0.70      0.88      0.78       194\n       Brown       0.61      0.69      0.65       213\n\n    accuracy                           0.72      2200\n   macro avg       0.72      0.72      0.71      2200\nweighted avg       0.72      0.72      0.71      2200\n","metadata":{},"execution_count":null,"outputs":[]}]}