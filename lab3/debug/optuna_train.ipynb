{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import joblib\n",
    "import evaluate\n",
    "from time import time\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoTokenizer, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './'\n",
    "OUTPUT_DIR = ROOT_DIR + 'gen_search/'\n",
    "METRIC = evaluate.load(\"chrf\")\n",
    "MODEL_PATH = ''\n",
    "DEVICE = 'cuda:0'\n",
    "MAX_SEQ_LEN = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchUtils:\n",
    "    #\n",
    "    search_config = {\n",
    "        'beamsearch': lambda trial: {\n",
    "            'num_beams': trial.suggest_categorical(\"num_beams\", [8,10,12,14,16,18,20]),\n",
    "            'num_beam_groups': trial.suggest_categorical('num_beam_groups', [2,4]),\n",
    "            'do_sample': trial.suggest_categorical('do_sample', [True, False])},\n",
    "\n",
    "        'sampling': lambda trial: {\n",
    "            'do_sample': True,\n",
    "            'top_k': trial.suggest_int('top_k', 0, 51, 5),\n",
    "            'top_p': trial.suggest_float('top_p',0.5,0.96,0.05)},\n",
    "\n",
    "        'contrastivesearch': lambda trial: {\n",
    "            'top_k': trial.suggest_int('top_k', 2, 50, 2),\n",
    "            'penalty_alpha': trial.suggest_float('penalty_alpha',0.5,0.9,0.05)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    #\n",
    "    def __init__(self, model_path, refs_file, max_length) -> None:\n",
    "        self.init_model(model_path)\n",
    "        self.formate_refs(refs_file)\n",
    "        self.generate_type = None\n",
    "        self.max_gen_seq_len = max_length\n",
    "        self.trial_counter = None\n",
    "\n",
    "    #\n",
    "    def formate_refs(self, refs_file):\n",
    "        df = pd.read_csv(refs_file, sep=';')\n",
    "\n",
    "        #\n",
    "        self.prompt_texts = [f'Название: \"{df[\"title\"][i]}\". Стихотворение:' for i in range(df.shape[0])]\n",
    "        #\n",
    "        self.ref_texts = [[f\"{prompt_t} {self.df['text'][i]}\"] for i, prompt_t in enumerate(self.prompt_texts)]\n",
    "        #\n",
    "        s_time = time()\n",
    "        self.enc_prompts = [self.tokenizer(prompt_t, return_tensors='pt', add_special_tokens=False) \n",
    "                            for prompt_t in self.prompt_texts]\n",
    "        e_time = time()\n",
    "        print(f\"Elapsed time: {round(e_time-s_time, 5)} sec.\")\n",
    "\n",
    "    # \n",
    "    def init_model(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        s_time = time()\n",
    "        self.model = AutoGPTQForCausalLM.from_quantized(model_path, device=DEVICE, \n",
    "                                                        use_safetensors=True)\n",
    "        e_time = time()\n",
    "        print(f\"Elapsed time: {round(e_time-s_time, 5)} sec.\")\n",
    "\n",
    "    #\n",
    "    def compute_metrics(self, refs, preds):\n",
    "        return METRIC(predictions=preds, \n",
    "                      references=refs, word_order=2)['score']\n",
    "\n",
    "    #\n",
    "    def generate_samples(self, enc_prompts, params):\n",
    "        generated_texts = []\n",
    "        for i, enc_prompt in tqdm(enumerate(enc_prompts)):\n",
    "            enc_prompt = {k: v.to(DEVICE) for k,v in enc_prompt.items()}\n",
    "\n",
    "            gen_encode = self.model.generate(max_length=self.max_gen_seq_len, \n",
    "                                             num_return_sequences=1, **enc_prompt,**params)\n",
    "            \n",
    "            gen_txt = self.tokenizer.batch_decode(gen_encode, skip_special_tokens=True)\n",
    "            \n",
    "            print(f\"== [trial - {self.trial_counter} | sample - {i}] GENERATED TEXT START: \")\n",
    "            print(gen_txt)\n",
    "            print(f\"== [{i}] GENERATED TEXT END ==\")\n",
    "            \n",
    "            generated_texts += gen_txt\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return generated_texts\n",
    "    \n",
    "    #\n",
    "    def search_objective(self, trials):\n",
    "        self.trial_counter += 1\n",
    "\n",
    "        #\n",
    "        selected_params = self.search_config[self.generate_type](trials)\n",
    "        #\n",
    "        pred_texts = self.generate_samples(self.enc_prompts, \n",
    "                                           selected_params)\n",
    "        #\n",
    "        charf_score = self.compute_metrics(self.ref_texts, \n",
    "                                           pred_texts)\n",
    "\n",
    "        return charf_score\n",
    "    \n",
    "    #\n",
    "    def save_search(self, study, study_name):\n",
    "        joblib.dump(study, f'{OUTPUT_DIR}{study_name}_study.pkl')\n",
    "\n",
    "#\n",
    "def search(study_name, utils, n_trials):\n",
    "    print(f\"====== START SEARCH ({study_name}) ======\")\n",
    "\n",
    "    study = optuna.create_study(directions=[\"maximize\"])\n",
    "    utils.generate_type = study_name\n",
    "    utils.trial_counter = 0\n",
    "    study.optimize(utils.search_objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(f\"\\n== TRIALS:\")\n",
    "    print(study.trials)\n",
    "    print(\"==\\n\")\n",
    "\n",
    "    print(f\"====== END SEARCH ({study_name}) ======\")\n",
    "\n",
    "    utils.save_search(study, study_name)\n",
    "\n",
    "    print(f\"====== SEARCH SAVED ======\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils = SearchUtils(MODEL_PATH, MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"beamsearch\", utils, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"sampling\", utils, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"contrastivesearch\", utils, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.suggest_float('diversity_penalty', 0.1, 10, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_objective(trials):\n",
    "    trials.suggest_float('diversity_penalty', 0.1, 10, step=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-29 13:24:55,204] A new study created in memory with name: no-name-720b9f78-a304-4d6f-9314-b714fd02ccbb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-29 13:24:55,377] Trial 0 failed with parameters: {'diversity_penalty': 0.30000000000000004} because of the following error: The value None could not be cast to float..\n",
      "[W 2023-12-29 13:24:55,379] Trial 0 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(directions=[\"maximize\"])\n",
    "study.optimize(test_objective, n_trials=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formate GenConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    #TODO\n",
    ")\n",
    "\n",
    "generation_config.save_pretrained(\"../pretrained_models/fffrrt_ruGPT-3.5-13B-GPTQ4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
