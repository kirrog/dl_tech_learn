{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from auto_gptq import AutoGPTQForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The safetensors archive passed at ./pretrained_models/fffrrt_ruGPT-3.5-13B-GPTQ4/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "GPT2GPTQForCausalLM hasn't fused attention module yet, will skip inject fused attention.\n",
      "GPT2GPTQForCausalLM hasn't fused mlp module yet, will skip inject fused mlp.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m./pretrained_models/fffrrt_ruGPT-3.5-13B-GPTQ4\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m peft_model_id \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./pretrained_models/ai3_gpt3_5_4bit_adapter_epoch17\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[39m=\u001b[39m AutoGPTQForCausalLM\u001b[39m.\u001b[39;49mfrom_quantized(model_id, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_safetensors\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39mload_adapter(peft_model_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/modeling/auto.py:129\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# TODO: do we need this filtering of kwargs? @PanQiWei is there a reason we can't just pass all kwargs?\u001b[39;00m\n\u001b[1;32m    124\u001b[0m keywords \u001b[39m=\u001b[39m {\n\u001b[1;32m    125\u001b[0m     key: kwargs[key]\n\u001b[1;32m    126\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(signature(quant_func)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m huggingface_kwargs\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs\n\u001b[1;32m    128\u001b[0m }\n\u001b[0;32m--> 129\u001b[0m \u001b[39mreturn\u001b[39;00m quant_func(\n\u001b[1;32m    130\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m    131\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    132\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    133\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    134\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m    135\u001b[0m     use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m    136\u001b[0m     inject_fused_attention\u001b[39m=\u001b[39;49minject_fused_attention,\n\u001b[1;32m    137\u001b[0m     inject_fused_mlp\u001b[39m=\u001b[39;49minject_fused_mlp,\n\u001b[1;32m    138\u001b[0m     use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    139\u001b[0m     quantize_config\u001b[39m=\u001b[39;49mquantize_config,\n\u001b[1;32m    140\u001b[0m     model_basename\u001b[39m=\u001b[39;49mmodel_basename,\n\u001b[1;32m    141\u001b[0m     use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m    142\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    143\u001b[0m     warmup_triton\u001b[39m=\u001b[39;49mwarmup_triton,\n\u001b[1;32m    144\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    145\u001b[0m     disable_exllama\u001b[39m=\u001b[39;49mdisable_exllama,\n\u001b[1;32m    146\u001b[0m     disable_exllamav2\u001b[39m=\u001b[39;49mdisable_exllamav2,\n\u001b[1;32m    147\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkeywords\n\u001b[1;32m    148\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/modeling/_base.py:1040\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, use_qigen, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfused_mlp_module_type\u001b[39m.\u001b[39minject_to_model(\n\u001b[1;32m   1035\u001b[0m             model,\n\u001b[1;32m   1036\u001b[0m             use_triton\u001b[39m=\u001b[39muse_triton\n\u001b[1;32m   1037\u001b[0m         )\n\u001b[1;32m   1039\u001b[0m \u001b[39m# Any post-initialization that require device information, for example buffers initialization on device.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m model \u001b[39m=\u001b[39m autogptq_post_init(model, use_act_order\u001b[39m=\u001b[39;49mquantize_config\u001b[39m.\u001b[39;49mdesc_act)\n\u001b[1;32m   1042\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m   1043\u001b[0m \u001b[39m# == step6: (optional) warmup triton == #\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/modeling/_utils.py:380\u001b[0m, in \u001b[0;36mautogptq_post_init\u001b[0;34m(model, use_act_order, max_input_length)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(submodule, \u001b[39m\"\u001b[39m\u001b[39mQUANT_TYPE\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m submodule\u001b[39m.\u001b[39mQUANT_TYPE \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexllamav2\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    379\u001b[0m             device \u001b[39m=\u001b[39m submodule\u001b[39m.\u001b[39mqweight\u001b[39m.\u001b[39mdevice\n\u001b[0;32m--> 380\u001b[0m             submodule\u001b[39m.\u001b[39;49mpost_init(temp_dq \u001b[39m=\u001b[39;49m model\u001b[39m.\u001b[39;49mdevice_tensors[device])\n\u001b[1;32m    381\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m    383\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/nn_modules/qlinear/qlinear_exllamav2.py:140\u001b[0m, in \u001b[0;36mQuantLinear.post_init\u001b[0;34m(self, temp_dq)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost_init\u001b[39m(\u001b[39mself\u001b[39m, temp_dq):\n\u001b[0;32m--> 140\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqweight\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqweight\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mindex \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_tensors \u001b[39m=\u001b[39m {\n\u001b[1;32m    143\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mqweight\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqweight,\n\u001b[1;32m    144\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mqzeros\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqzeros,\n\u001b[1;32m    145\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mscales\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscales,\n\u001b[1;32m    146\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mg_idx\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_idx\n\u001b[1;32m    147\u001b[0m     }\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_id = \"./pretrained_models/fffrrt_ruGPT-3.5-13B-GPTQ4\"\n",
    "peft_model_id = './pretrained_models/ai3_gpt3_5_4bit_adapter_epoch17'\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_id, device=\"cpu\", use_safetensors=True)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
