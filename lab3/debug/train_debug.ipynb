{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import IA3Model, IA3Config, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from auto_gptq import AutoGPTQForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE:  1\n",
      "LR:  0.0003\n",
      "EPOCHS:  12\n",
      "DEVICE:  cpu\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = '/home/dzigen/Desktop/ITMO/sem1/DLtech/dl_tech_learn/lab3/pretrained_models/ruGPT3_5_13B_8bit'\n",
    "OUTPUT_DIR = './'\n",
    "TRAIN_FILE = '/home/dzigen/Desktop/ITMO/sem1/DLtech/dl_tech_learn/lab3/data/train_part.csv'\n",
    "EVAL_FILE = '/home/dzigen/Desktop/ITMO/sem1/DLtech/dl_tech_learn/lab3/data/test_part.csv'\n",
    "LOG_DIR = OUTPUT_DIR + 'logs/'\n",
    "ADAPTER_DIR = LOG_DIR + 'ai3_gpt3_5_8bit_adapter'\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 12\n",
    "LR = 3e-4\n",
    "\n",
    "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
    "print(\"LR: \", LR)\n",
    "print(\"EPOCHS: \", EPOCHS)\n",
    "print(\"DEVICE: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "QuantLinear() does not have a parameter or a buffer named weight.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m AutoGPTQForCausalLM\u001b[39m.\u001b[39;49mfrom_quantized(BASE_MODEL, device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/modeling/auto.py:129\u001b[0m, in \u001b[0;36mAutoGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# TODO: do we need this filtering of kwargs? @PanQiWei is there a reason we can't just pass all kwargs?\u001b[39;00m\n\u001b[1;32m    124\u001b[0m keywords \u001b[39m=\u001b[39m {\n\u001b[1;32m    125\u001b[0m     key: kwargs[key]\n\u001b[1;32m    126\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(signature(quant_func)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m huggingface_kwargs\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m kwargs\n\u001b[1;32m    128\u001b[0m }\n\u001b[0;32m--> 129\u001b[0m \u001b[39mreturn\u001b[39;00m quant_func(\n\u001b[1;32m    130\u001b[0m     model_name_or_path\u001b[39m=\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m    131\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    132\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    133\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    134\u001b[0m     low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m    135\u001b[0m     use_triton\u001b[39m=\u001b[39;49muse_triton,\n\u001b[1;32m    136\u001b[0m     inject_fused_attention\u001b[39m=\u001b[39;49minject_fused_attention,\n\u001b[1;32m    137\u001b[0m     inject_fused_mlp\u001b[39m=\u001b[39;49minject_fused_mlp,\n\u001b[1;32m    138\u001b[0m     use_cuda_fp16\u001b[39m=\u001b[39;49muse_cuda_fp16,\n\u001b[1;32m    139\u001b[0m     quantize_config\u001b[39m=\u001b[39;49mquantize_config,\n\u001b[1;32m    140\u001b[0m     model_basename\u001b[39m=\u001b[39;49mmodel_basename,\n\u001b[1;32m    141\u001b[0m     use_safetensors\u001b[39m=\u001b[39;49muse_safetensors,\n\u001b[1;32m    142\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    143\u001b[0m     warmup_triton\u001b[39m=\u001b[39;49mwarmup_triton,\n\u001b[1;32m    144\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[1;32m    145\u001b[0m     disable_exllama\u001b[39m=\u001b[39;49mdisable_exllama,\n\u001b[1;32m    146\u001b[0m     disable_exllamav2\u001b[39m=\u001b[39;49mdisable_exllamav2,\n\u001b[1;32m    147\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkeywords\n\u001b[1;32m    148\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/modeling/_base.py:946\u001b[0m, in \u001b[0;36mBaseGPTQForCausalLM.from_quantized\u001b[0;34m(cls, model_name_or_path, device_map, max_memory, device, low_cpu_mem_usage, use_triton, use_qigen, torch_dtype, inject_fused_attention, inject_fused_mlp, use_cuda_fp16, quantize_config, model_basename, use_safetensors, trust_remote_code, warmup_triton, trainable, disable_exllama, disable_exllamav2, **kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m     original_set_module_tensor_to_device \u001b[39m=\u001b[39m accelerate\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmodeling\u001b[39m.\u001b[39mset_module_tensor_to_device\n\u001b[1;32m    944\u001b[0m     accelerate\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmodeling\u001b[39m.\u001b[39mset_module_tensor_to_device \u001b[39m=\u001b[39m set_module_tensor_to_device_patched\n\u001b[0;32m--> 946\u001b[0m accelerate\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mmodeling\u001b[39m.\u001b[39;49mload_checkpoint_in_model(\n\u001b[1;32m    947\u001b[0m     model,\n\u001b[1;32m    948\u001b[0m     dtype\u001b[39m=\u001b[39;49mtorch_dtype,  \u001b[39m# This is very hacky but works due to https://github.com/huggingface/accelerate/blob/bd72a5f1a80d5146554458823f8aeda0a9db5297/src/accelerate/utils/modeling.py#L292\u001b[39;49;00m\n\u001b[1;32m    949\u001b[0m     checkpoint\u001b[39m=\u001b[39;49mmodel_save_name,\n\u001b[1;32m    950\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    951\u001b[0m     offload_state_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    952\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    953\u001b[0m )\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(accelerate\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m0.24.99\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m accelerate\u001b[39m.\u001b[39m__version__ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.25.0.dev0\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    956\u001b[0m     accelerate\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmodeling\u001b[39m.\u001b[39mset_module_tensor_to_device \u001b[39m=\u001b[39m original_set_module_tensor_to_device\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/utils/modeling.py:1411\u001b[0m, in \u001b[0;36mload_checkpoint_in_model\u001b[0;34m(model, checkpoint, device_map, offload_folder, dtype, offload_state_dict, offload_buffers, keep_in_fp32_modules, offload_8bit_bnb)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         quantize_and_offload_8bit(\n\u001b[1;32m   1408\u001b[0m             model, param, param_name, new_dtype, state_dict_folder, state_dict_index, fp16_statistics\n\u001b[1;32m   1409\u001b[0m         )\n\u001b[1;32m   1410\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1411\u001b[0m         set_module_tensor_to_device(model, param_name, \u001b[39m\"\u001b[39;49m\u001b[39mmeta\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mnew_dtype)\n\u001b[1;32m   1412\u001b[0m         offload_weight(param, param_name, state_dict_folder, index\u001b[39m=\u001b[39mstate_dict_index)\n\u001b[1;32m   1413\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/auto_gptq/utils/patch_utils.py:44\u001b[0m, in \u001b[0;36mset_module_tensor_to_device_patched\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m     41\u001b[0m     tensor_name \u001b[39m=\u001b[39m splits[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m tensor_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_parameters \u001b[39mand\u001b[39;00m tensor_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_buffers:\n\u001b[0;32m---> 44\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m does not have a parameter or a buffer named \u001b[39m\u001b[39m{\u001b[39;00mtensor_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m is_buffer \u001b[39m=\u001b[39m tensor_name \u001b[39min\u001b[39;00m module\u001b[39m.\u001b[39m_buffers\n\u001b[1;32m     46\u001b[0m old_value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, tensor_name)\n",
      "\u001b[0;31mValueError\u001b[0m: QuantLinear() does not have a parameter or a buffer named weight."
     ]
    }
   ],
   "source": [
    "model = AutoGPTQForCausalLM.from_quantized(BASE_MODEL, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/dzigen/Desktop/ITMO/sem1/DLtech/dl_tech_learn/lab3/pretrained_models/ruGPT3_5_13B_8bit were not used when initializing GPT2LMHeadModel: ['transformer.h.33.attn.c_attn.qzeros', 'transformer.h.12.attn.c_attn.scales', 'transformer.h.28.attn.c_proj.qweight', 'transformer.h.27.attn.c_attn.g_idx', 'transformer.h.1.mlp.c_proj.scales', 'transformer.h.1.mlp.c_proj.g_idx', 'transformer.h.28.attn.c_proj.qzeros', 'transformer.h.33.mlp.c_fc.g_idx', 'transformer.h.29.attn.c_proj.scales', 'transformer.h.0.mlp.c_proj.qzeros', 'transformer.h.35.attn.c_proj.qweight', 'transformer.h.20.attn.c_attn.qzeros', 'transformer.h.32.mlp.c_fc.g_idx', 'transformer.h.26.mlp.c_proj.g_idx', 'transformer.h.4.mlp.c_fc.qzeros', 'transformer.h.16.mlp.c_fc.scales', 'transformer.h.37.mlp.c_proj.scales', 'transformer.h.14.attn.c_attn.qweight', 'transformer.h.33.mlp.c_proj.qweight', 'transformer.h.12.attn.c_proj.g_idx', 'transformer.h.2.mlp.c_proj.qzeros', 'transformer.h.4.attn.c_attn.qweight', 'transformer.h.11.attn.c_attn.g_idx', 'transformer.h.2.mlp.c_proj.scales', 'transformer.h.29.mlp.c_proj.g_idx', 'transformer.h.38.mlp.c_fc.qweight', 'transformer.h.28.mlp.c_proj.qweight', 'transformer.h.37.mlp.c_fc.qzeros', 'transformer.h.39.mlp.c_fc.qzeros', 'transformer.h.31.attn.c_proj.g_idx', 'transformer.h.33.attn.c_attn.g_idx', 'transformer.h.4.mlp.c_proj.qzeros', 'transformer.h.29.mlp.c_proj.scales', 'transformer.h.6.attn.c_attn.scales', 'transformer.h.33.mlp.c_fc.qzeros', 'transformer.h.38.mlp.c_proj.qzeros', 'transformer.h.11.mlp.c_proj.qweight', 'transformer.h.22.attn.c_attn.qweight', 'transformer.h.18.mlp.c_proj.qweight', 'transformer.h.29.mlp.c_fc.scales', 'transformer.h.30.attn.c_proj.g_idx', 'transformer.h.33.mlp.c_fc.qweight', 'transformer.h.24.attn.c_proj.scales', 'transformer.h.23.mlp.c_fc.scales', 'transformer.h.6.mlp.c_proj.qzeros', 'transformer.h.12.mlp.c_proj.qzeros', 'transformer.h.21.attn.c_proj.qweight', 'transformer.h.38.attn.c_attn.g_idx', 'transformer.h.13.attn.c_attn.g_idx', 'transformer.h.30.mlp.c_proj.qzeros', 'transformer.h.34.attn.c_proj.qzeros', 'transformer.h.39.attn.c_proj.g_idx', 'transformer.h.3.mlp.c_proj.scales', 'transformer.h.7.attn.c_attn.qzeros', 'transformer.h.10.mlp.c_fc.qzeros', 'transformer.h.9.mlp.c_proj.qzeros', 'transformer.h.6.mlp.c_fc.qweight', 'transformer.h.6.attn.c_proj.qzeros', 'transformer.h.6.mlp.c_proj.scales', 'transformer.h.22.attn.c_proj.qweight', 'transformer.h.4.attn.c_attn.scales', 'transformer.h.29.attn.c_proj.qweight', 'transformer.h.32.attn.c_proj.g_idx', 'transformer.h.20.mlp.c_fc.g_idx', 'transformer.h.8.attn.c_attn.qzeros', 'transformer.h.26.mlp.c_fc.g_idx', 'transformer.h.36.attn.c_attn.qweight', 'transformer.h.1.mlp.c_proj.qweight', 'transformer.h.27.mlp.c_proj.scales', 'transformer.h.25.attn.c_attn.g_idx', 'transformer.h.27.attn.c_attn.qzeros', 'transformer.h.11.attn.c_attn.scales', 'transformer.h.23.mlp.c_proj.qzeros', 'transformer.h.4.attn.c_proj.g_idx', 'transformer.h.2.mlp.c_proj.g_idx', 'transformer.h.38.attn.c_attn.qzeros', 'transformer.h.4.attn.c_proj.qweight', 'transformer.h.7.attn.c_proj.qweight', 'transformer.h.34.attn.c_attn.scales', 'transformer.h.18.mlp.c_proj.g_idx', 'transformer.h.13.attn.c_proj.g_idx', 'transformer.h.38.mlp.c_fc.qzeros', 'transformer.h.9.attn.c_proj.qweight', 'transformer.h.12.attn.c_attn.g_idx', 'transformer.h.30.mlp.c_fc.scales', 'transformer.h.37.mlp.c_proj.qweight', 'transformer.h.5.mlp.c_fc.qweight', 'transformer.h.26.mlp.c_proj.qweight', 'transformer.h.24.mlp.c_fc.scales', 'transformer.h.33.mlp.c_fc.scales', 'transformer.h.20.mlp.c_fc.qzeros', 'transformer.h.23.mlp.c_proj.g_idx', 'transformer.h.14.attn.c_proj.qzeros', 'transformer.h.36.mlp.c_fc.qweight', 'transformer.h.19.mlp.c_proj.g_idx', 'transformer.h.25.attn.c_attn.qweight', 'transformer.h.29.attn.c_attn.qzeros', 'transformer.h.32.mlp.c_fc.qweight', 'transformer.h.21.mlp.c_fc.qweight', 'transformer.h.13.attn.c_attn.qzeros', 'transformer.h.17.mlp.c_fc.qweight', 'transformer.h.8.mlp.c_proj.scales', 'transformer.h.16.attn.c_attn.qweight', 'transformer.h.3.attn.c_proj.g_idx', 'transformer.h.12.mlp.c_fc.scales', 'transformer.h.27.mlp.c_proj.g_idx', 'transformer.h.2.attn.c_proj.qzeros', 'transformer.h.32.mlp.c_fc.qzeros', 'transformer.h.10.mlp.c_proj.g_idx', 'transformer.h.24.attn.c_attn.g_idx', 'transformer.h.8.attn.c_attn.g_idx', 'transformer.h.1.attn.c_proj.qweight', 'transformer.h.16.mlp.c_fc.g_idx', 'transformer.h.33.attn.c_proj.g_idx', 'transformer.h.35.mlp.c_proj.g_idx', 'transformer.h.2.mlp.c_fc.scales', 'transformer.h.10.attn.c_proj.g_idx', 'transformer.h.28.mlp.c_fc.g_idx', 'transformer.h.14.attn.c_attn.g_idx', 'transformer.h.0.mlp.c_fc.scales', 'transformer.h.0.mlp.c_fc.qzeros', 'transformer.h.36.mlp.c_proj.qzeros', 'transformer.h.2.attn.c_proj.g_idx', 'transformer.h.27.attn.c_attn.qweight', 'transformer.h.38.attn.c_proj.scales', 'transformer.h.14.mlp.c_fc.g_idx', 'transformer.h.34.mlp.c_fc.g_idx', 'transformer.h.6.mlp.c_proj.g_idx', 'transformer.h.0.attn.c_attn.scales', 'transformer.h.7.attn.c_attn.g_idx', 'transformer.h.39.mlp.c_proj.qweight', 'transformer.h.6.mlp.c_fc.scales', 'transformer.h.14.attn.c_proj.qweight', 'transformer.h.4.attn.c_proj.qzeros', 'transformer.h.13.mlp.c_fc.qweight', 'transformer.h.1.mlp.c_fc.qzeros', 'transformer.h.0.attn.c_attn.qzeros', 'transformer.h.27.mlp.c_fc.g_idx', 'transformer.h.39.mlp.c_proj.qzeros', 'transformer.h.36.attn.c_proj.qzeros', 'transformer.h.7.attn.c_proj.scales', 'transformer.h.22.mlp.c_fc.qzeros', 'transformer.h.31.attn.c_proj.qzeros', 'transformer.h.36.mlp.c_fc.qzeros', 'transformer.h.0.attn.c_proj.g_idx', 'transformer.h.30.attn.c_proj.qweight', 'transformer.h.19.attn.c_attn.qweight', 'transformer.h.35.attn.c_attn.qweight', 'transformer.h.33.attn.c_attn.scales', 'transformer.h.18.attn.c_attn.qzeros', 'transformer.h.1.mlp.c_proj.qzeros', 'transformer.h.36.attn.c_proj.g_idx', 'transformer.h.8.mlp.c_fc.scales', 'transformer.h.16.mlp.c_fc.qweight', 'transformer.h.13.attn.c_attn.qweight', 'transformer.h.18.attn.c_attn.scales', 'transformer.h.24.attn.c_proj.g_idx', 'transformer.h.24.attn.c_proj.qzeros', 'transformer.h.38.attn.c_attn.qweight', 'transformer.h.9.mlp.c_fc.scales', 'transformer.h.16.mlp.c_proj.qweight', 'transformer.h.11.attn.c_attn.qzeros', 'transformer.h.34.attn.c_attn.qzeros', 'transformer.h.27.mlp.c_proj.qzeros', 'transformer.h.19.attn.c_proj.g_idx', 'transformer.h.11.mlp.c_fc.qzeros', 'transformer.h.14.attn.c_proj.scales', 'transformer.h.27.attn.c_proj.g_idx', 'transformer.h.29.attn.c_proj.qzeros', 'transformer.h.11.attn.c_proj.qzeros', 'transformer.h.2.attn.c_proj.qweight', 'transformer.h.22.mlp.c_fc.scales', 'transformer.h.4.mlp.c_proj.qweight', 'transformer.h.14.attn.c_attn.qzeros', 'transformer.h.22.mlp.c_proj.g_idx', 'transformer.h.36.mlp.c_proj.qweight', 'transformer.h.18.mlp.c_proj.qzeros', 'transformer.h.9.mlp.c_fc.g_idx', 'transformer.h.30.attn.c_attn.scales', 'transformer.h.15.attn.c_attn.scales', 'transformer.h.23.attn.c_proj.qweight', 'transformer.h.35.mlp.c_proj.qweight', 'transformer.h.24.mlp.c_fc.g_idx', 'transformer.h.37.attn.c_attn.qzeros', 'transformer.h.37.attn.c_proj.qzeros', 'transformer.h.3.attn.c_proj.scales', 'transformer.h.20.mlp.c_proj.g_idx', 'transformer.h.28.attn.c_attn.g_idx', 'transformer.h.9.attn.c_attn.g_idx', 'transformer.h.39.attn.c_attn.g_idx', 'transformer.h.3.mlp.c_fc.scales', 'transformer.h.20.mlp.c_proj.qzeros', 'transformer.h.21.mlp.c_fc.scales', 'transformer.h.11.mlp.c_fc.qweight', 'transformer.h.39.mlp.c_proj.g_idx', 'transformer.h.6.attn.c_proj.qweight', 'transformer.h.37.attn.c_proj.scales', 'transformer.h.12.mlp.c_fc.qzeros', 'transformer.h.27.mlp.c_fc.qweight', 'transformer.h.25.mlp.c_proj.qzeros', 'transformer.h.14.mlp.c_proj.qweight', 'transformer.h.2.mlp.c_proj.qweight', 'transformer.h.29.attn.c_proj.g_idx', 'transformer.h.5.mlp.c_proj.g_idx', 'transformer.h.10.attn.c_proj.scales', 'transformer.h.14.mlp.c_proj.qzeros', 'transformer.h.20.attn.c_proj.scales', 'transformer.h.36.attn.c_proj.qweight', 'transformer.h.18.mlp.c_fc.qzeros', 'transformer.h.39.mlp.c_fc.g_idx', 'transformer.h.4.attn.c_attn.qzeros', 'transformer.h.21.attn.c_attn.qzeros', 'transformer.h.39.attn.c_proj.qweight', 'transformer.h.32.attn.c_proj.qzeros', 'transformer.h.20.mlp.c_proj.scales', 'transformer.h.10.attn.c_proj.qweight', 'transformer.h.16.attn.c_proj.scales', 'transformer.h.23.mlp.c_fc.qzeros', 'transformer.h.30.attn.c_attn.qweight', 'transformer.h.6.mlp.c_fc.qzeros', 'transformer.h.10.attn.c_attn.g_idx', 'transformer.h.25.mlp.c_proj.qweight', 'transformer.h.25.mlp.c_proj.g_idx', 'transformer.h.39.attn.c_attn.scales', 'transformer.h.27.attn.c_proj.qzeros', 'transformer.h.18.attn.c_proj.qzeros', 'transformer.h.1.mlp.c_fc.scales', 'transformer.h.19.mlp.c_proj.scales', 'transformer.h.34.attn.c_proj.scales', 'transformer.h.36.attn.c_proj.scales', 'transformer.h.31.mlp.c_fc.qzeros', 'transformer.h.24.mlp.c_fc.qweight', 'transformer.h.12.mlp.c_proj.g_idx', 'transformer.h.16.attn.c_attn.scales', 'transformer.h.37.mlp.c_fc.g_idx', 'transformer.h.9.attn.c_attn.qweight', 'transformer.h.23.attn.c_attn.qzeros', 'transformer.h.13.attn.c_proj.qweight', 'transformer.h.14.mlp.c_fc.qzeros', 'transformer.h.7.mlp.c_proj.qweight', 'transformer.h.4.mlp.c_fc.qweight', 'transformer.h.2.attn.c_attn.g_idx', 'transformer.h.32.attn.c_attn.g_idx', 'transformer.h.38.attn.c_proj.qweight', 'transformer.h.8.mlp.c_fc.qweight', 'transformer.h.8.mlp.c_proj.qzeros', 'transformer.h.28.attn.c_attn.scales', 'transformer.h.11.mlp.c_proj.scales', 'transformer.h.35.attn.c_attn.scales', 'transformer.h.35.mlp.c_proj.scales', 'transformer.h.2.attn.c_attn.qweight', 'transformer.h.33.attn.c_proj.scales', 'transformer.h.31.attn.c_attn.g_idx', 'transformer.h.24.attn.c_attn.qweight', 'transformer.h.1.attn.c_attn.qzeros', 'transformer.h.8.attn.c_proj.qweight', 'transformer.h.29.mlp.c_fc.g_idx', 'transformer.h.33.mlp.c_proj.g_idx', 'transformer.h.2.attn.c_attn.qzeros', 'transformer.h.18.attn.c_proj.g_idx', 'transformer.h.26.mlp.c_fc.qzeros', 'transformer.h.6.attn.c_proj.g_idx', 'transformer.h.29.mlp.c_proj.qweight', 'transformer.h.7.mlp.c_proj.scales', 'transformer.h.36.attn.c_attn.qzeros', 'transformer.h.12.attn.c_proj.scales', 'transformer.h.26.mlp.c_fc.scales', 'transformer.h.7.attn.c_attn.qweight', 'transformer.h.31.mlp.c_fc.qweight', 'transformer.h.30.attn.c_attn.g_idx', 'transformer.h.4.attn.c_attn.g_idx', 'transformer.h.28.mlp.c_proj.g_idx', 'transformer.h.14.mlp.c_proj.g_idx', 'transformer.h.15.attn.c_proj.qzeros', 'transformer.h.28.mlp.c_proj.scales', 'transformer.h.25.mlp.c_fc.qzeros', 'transformer.h.17.mlp.c_proj.g_idx', 'transformer.h.26.attn.c_attn.g_idx', 'transformer.h.38.mlp.c_proj.scales', 'transformer.h.29.attn.c_attn.g_idx', 'transformer.h.15.attn.c_proj.qweight', 'transformer.h.22.attn.c_attn.g_idx', 'transformer.h.36.mlp.c_proj.g_idx', 'transformer.h.26.attn.c_proj.qzeros', 'transformer.h.37.mlp.c_proj.qzeros', 'transformer.h.3.attn.c_attn.scales', 'transformer.h.0.mlp.c_fc.g_idx', 'transformer.h.16.attn.c_attn.g_idx', 'transformer.h.2.attn.c_attn.scales', 'transformer.h.13.attn.c_proj.qzeros', 'transformer.h.9.attn.c_proj.qzeros', 'transformer.h.32.attn.c_attn.qweight', 'transformer.h.16.mlp.c_proj.g_idx', 'transformer.h.5.attn.c_proj.scales', 'transformer.h.10.mlp.c_fc.scales', 'transformer.h.29.attn.c_attn.qweight', 'transformer.h.24.mlp.c_proj.g_idx', 'transformer.h.13.attn.c_attn.scales', 'transformer.h.25.attn.c_proj.scales', 'transformer.h.26.attn.c_proj.qweight', 'transformer.h.11.mlp.c_proj.qzeros', 'transformer.h.7.mlp.c_fc.qweight', 'transformer.h.25.attn.c_attn.qzeros', 'transformer.h.38.mlp.c_proj.qweight', 'transformer.h.12.mlp.c_proj.qweight', 'transformer.h.0.mlp.c_fc.qweight', 'transformer.h.14.attn.c_attn.scales', 'transformer.h.31.mlp.c_fc.scales', 'transformer.h.28.attn.c_attn.qzeros', 'transformer.h.31.mlp.c_proj.qweight', 'transformer.h.38.mlp.c_fc.scales', 'transformer.h.26.attn.c_proj.g_idx', 'transformer.h.3.attn.c_proj.qweight', 'transformer.h.34.attn.c_proj.qweight', 'transformer.h.9.attn.c_proj.g_idx', 'transformer.h.25.mlp.c_fc.qweight', 'transformer.h.4.mlp.c_proj.g_idx', 'transformer.h.37.mlp.c_fc.scales', 'transformer.h.8.mlp.c_fc.g_idx', 'transformer.h.16.attn.c_proj.qzeros', 'transformer.h.15.attn.c_attn.qzeros', 'transformer.h.31.attn.c_proj.scales', 'transformer.h.18.attn.c_attn.g_idx', 'transformer.h.6.attn.c_proj.scales', 'transformer.h.5.mlp.c_fc.g_idx', 'transformer.h.12.attn.c_proj.qweight', 'transformer.h.9.attn.c_attn.scales', 'transformer.h.20.attn.c_proj.qweight', 'transformer.h.6.mlp.c_proj.qweight', 'transformer.h.29.attn.c_attn.scales', 'transformer.h.1.attn.c_attn.scales', 'transformer.h.13.mlp.c_fc.scales', 'transformer.h.20.attn.c_attn.scales', 'transformer.h.30.mlp.c_proj.scales', 'transformer.h.23.attn.c_proj.g_idx', 'transformer.h.26.attn.c_attn.scales', 'transformer.h.15.mlp.c_fc.scales', 'transformer.h.13.mlp.c_fc.g_idx', 'transformer.h.10.attn.c_proj.qzeros', 'transformer.h.19.attn.c_proj.qweight', 'transformer.h.2.mlp.c_fc.qzeros', 'transformer.h.24.mlp.c_proj.qzeros', 'transformer.h.25.attn.c_attn.scales', 'transformer.h.8.attn.c_proj.g_idx', 'transformer.h.9.mlp.c_fc.qzeros', 'transformer.h.7.mlp.c_fc.qzeros', 'transformer.h.18.mlp.c_fc.qweight', 'transformer.h.34.mlp.c_proj.qzeros', 'transformer.h.23.attn.c_attn.g_idx', 'transformer.h.31.mlp.c_proj.g_idx', 'transformer.h.26.attn.c_attn.qweight', 'transformer.h.7.attn.c_proj.g_idx', 'transformer.h.32.attn.c_attn.qzeros', 'transformer.h.28.attn.c_proj.g_idx', 'transformer.h.37.mlp.c_proj.g_idx', 'transformer.h.34.mlp.c_fc.scales', 'transformer.h.1.attn.c_proj.scales', 'transformer.h.33.mlp.c_proj.scales', 'transformer.h.10.mlp.c_proj.qweight', 'transformer.h.20.mlp.c_fc.scales', 'transformer.h.26.mlp.c_proj.scales', 'transformer.h.35.mlp.c_fc.qweight', 'transformer.h.12.attn.c_attn.qzeros', 'transformer.h.36.mlp.c_fc.scales', 'transformer.h.20.mlp.c_fc.qweight', 'transformer.h.22.mlp.c_proj.qweight', 'transformer.h.39.attn.c_proj.scales', 'transformer.h.24.attn.c_attn.qzeros', 'transformer.h.20.attn.c_attn.g_idx', 'transformer.h.27.mlp.c_proj.qweight', 'transformer.h.32.mlp.c_proj.qweight', 'transformer.h.26.attn.c_proj.scales', 'transformer.h.30.mlp.c_fc.qzeros', 'transformer.h.25.mlp.c_fc.g_idx', 'transformer.h.26.mlp.c_proj.qzeros', 'transformer.h.36.attn.c_attn.scales', 'transformer.h.17.attn.c_proj.g_idx', 'transformer.h.5.attn.c_proj.qzeros', 'transformer.h.34.mlp.c_fc.qweight', 'transformer.h.24.mlp.c_proj.scales', 'transformer.h.23.attn.c_proj.qzeros', 'transformer.h.10.mlp.c_fc.g_idx', 'transformer.h.18.mlp.c_fc.g_idx', 'transformer.h.9.mlp.c_proj.g_idx', 'transformer.h.17.mlp.c_proj.qzeros', 'transformer.h.3.mlp.c_fc.qweight', 'transformer.h.7.mlp.c_proj.qzeros', 'transformer.h.1.attn.c_proj.qzeros', 'transformer.h.19.mlp.c_fc.qweight', 'transformer.h.28.mlp.c_fc.qzeros', 'transformer.h.11.mlp.c_proj.g_idx', 'transformer.h.32.mlp.c_proj.g_idx', 'transformer.h.7.attn.c_attn.scales', 'transformer.h.0.attn.c_proj.scales', 'transformer.h.6.mlp.c_fc.g_idx', 'transformer.h.9.attn.c_proj.scales', 'transformer.h.23.mlp.c_fc.g_idx', 'transformer.h.16.mlp.c_fc.qzeros', 'transformer.h.8.attn.c_attn.qweight', 'transformer.h.13.attn.c_proj.scales', 'transformer.h.3.attn.c_attn.g_idx', 'transformer.h.9.mlp.c_proj.scales', 'transformer.h.7.mlp.c_fc.scales', 'transformer.h.22.mlp.c_fc.g_idx', 'transformer.h.11.attn.c_attn.qweight', 'transformer.h.17.mlp.c_proj.scales', 'transformer.h.33.attn.c_proj.qweight', 'transformer.h.31.mlp.c_proj.qzeros', 'transformer.h.10.attn.c_attn.scales', 'transformer.h.22.attn.c_attn.scales', 'transformer.h.4.mlp.c_fc.g_idx', 'transformer.h.25.attn.c_proj.qweight', 'transformer.h.18.attn.c_attn.qweight', 'transformer.h.5.mlp.c_proj.scales', 'transformer.h.15.attn.c_attn.g_idx', 'transformer.h.19.attn.c_attn.scales', 'transformer.h.32.mlp.c_proj.qzeros', 'transformer.h.35.attn.c_proj.g_idx', 'transformer.h.12.attn.c_proj.qzeros', 'transformer.h.26.attn.c_attn.qzeros', 'transformer.h.35.mlp.c_fc.g_idx', 'transformer.h.20.attn.c_proj.g_idx', 'transformer.h.23.attn.c_proj.scales', 'transformer.h.17.attn.c_proj.qzeros', 'transformer.h.21.attn.c_proj.qzeros', 'transformer.h.24.attn.c_proj.qweight', 'transformer.h.18.mlp.c_proj.scales', 'transformer.h.17.attn.c_proj.scales', 'transformer.h.32.mlp.c_proj.scales', 'transformer.h.28.attn.c_attn.qweight', 'transformer.h.34.mlp.c_fc.qzeros', 'transformer.h.37.attn.c_attn.g_idx', 'transformer.h.0.mlp.c_proj.scales', 'transformer.h.1.attn.c_attn.qweight', 'transformer.h.17.mlp.c_fc.scales', 'transformer.h.21.mlp.c_proj.scales', 'transformer.h.27.mlp.c_fc.qzeros', 'transformer.h.38.attn.c_proj.g_idx', 'transformer.h.8.attn.c_proj.scales', 'transformer.h.21.mlp.c_proj.qweight', 'transformer.h.12.attn.c_attn.qweight', 'transformer.h.4.attn.c_proj.scales', 'transformer.h.15.mlp.c_fc.qweight', 'transformer.h.1.attn.c_attn.g_idx', 'transformer.h.13.mlp.c_proj.qweight', 'transformer.h.6.attn.c_attn.qzeros', 'transformer.h.24.mlp.c_proj.qweight', 'transformer.h.0.attn.c_attn.qweight', 'transformer.h.38.mlp.c_proj.g_idx', 'transformer.h.5.attn.c_proj.qweight', 'transformer.h.0.mlp.c_proj.g_idx', 'transformer.h.20.attn.c_proj.qzeros', 'transformer.h.35.attn.c_attn.qzeros', 'transformer.h.0.mlp.c_proj.qweight', 'transformer.h.19.mlp.c_proj.qweight', 'transformer.h.22.mlp.c_fc.qweight', 'transformer.h.27.attn.c_proj.qweight', 'transformer.h.19.mlp.c_fc.scales', 'transformer.h.28.mlp.c_fc.qweight', 'transformer.h.22.attn.c_proj.qzeros', 'transformer.h.11.attn.c_proj.qweight', 'transformer.h.35.mlp.c_proj.qzeros', 'transformer.h.33.attn.c_attn.qweight', 'transformer.h.39.mlp.c_fc.scales', 'transformer.h.10.attn.c_attn.qweight', 'transformer.h.22.mlp.c_proj.qzeros', 'transformer.h.16.attn.c_proj.g_idx', 'transformer.h.3.mlp.c_proj.qzeros', 'transformer.h.7.attn.c_proj.qzeros', 'transformer.h.16.attn.c_attn.qzeros', 'transformer.h.37.attn.c_attn.qweight', 'transformer.h.3.attn.c_attn.qzeros', 'transformer.h.38.mlp.c_fc.g_idx', 'transformer.h.38.attn.c_proj.qzeros', 'transformer.h.37.attn.c_attn.scales', 'transformer.h.17.attn.c_attn.scales', 'transformer.h.17.mlp.c_fc.qzeros', 'transformer.h.21.attn.c_attn.qweight', 'transformer.h.3.attn.c_proj.qzeros', 'transformer.h.21.mlp.c_proj.qzeros', 'transformer.h.17.attn.c_proj.qweight', 'transformer.h.1.mlp.c_fc.g_idx', 'transformer.h.37.mlp.c_fc.qweight', 'transformer.h.20.attn.c_attn.qweight', 'transformer.h.30.attn.c_proj.scales', 'transformer.h.11.attn.c_proj.g_idx', 'transformer.h.15.mlp.c_proj.scales', 'transformer.h.23.attn.c_attn.qweight', 'transformer.h.21.attn.c_attn.g_idx', 'transformer.h.2.mlp.c_fc.g_idx', 'transformer.h.22.mlp.c_proj.scales', 'transformer.h.12.mlp.c_fc.g_idx', 'transformer.h.10.mlp.c_proj.qzeros', 'transformer.h.21.mlp.c_fc.g_idx', 'transformer.h.8.mlp.c_proj.g_idx', 'transformer.h.3.mlp.c_fc.g_idx', 'transformer.h.28.mlp.c_proj.qzeros', 'transformer.h.4.mlp.c_fc.scales', 'transformer.h.31.attn.c_attn.qweight', 'transformer.h.21.attn.c_proj.g_idx', 'transformer.h.19.attn.c_attn.qzeros', 'transformer.h.21.mlp.c_proj.g_idx', 'transformer.h.16.attn.c_proj.qweight', 'transformer.h.24.attn.c_attn.scales', 'transformer.h.31.attn.c_attn.scales', 'transformer.h.8.mlp.c_fc.qzeros', 'transformer.h.35.mlp.c_fc.qzeros', 'transformer.h.28.mlp.c_fc.scales', 'transformer.h.22.attn.c_proj.g_idx', 'transformer.h.37.attn.c_proj.g_idx', 'transformer.h.15.mlp.c_proj.qweight', 'transformer.h.24.mlp.c_fc.qzeros', 'transformer.h.14.attn.c_proj.g_idx', 'transformer.h.35.attn.c_proj.qzeros', 'transformer.h.39.attn.c_attn.qweight', 'transformer.h.23.attn.c_attn.scales', 'transformer.h.23.mlp.c_proj.qweight', 'transformer.h.9.mlp.c_proj.qweight', 'transformer.h.18.attn.c_proj.qweight', 'transformer.h.10.mlp.c_proj.scales', 'transformer.h.5.attn.c_attn.g_idx', 'transformer.h.14.mlp.c_proj.scales', 'transformer.h.19.mlp.c_fc.g_idx', 'transformer.h.25.attn.c_proj.qzeros', 'transformer.h.21.attn.c_proj.scales', 'transformer.h.12.mlp.c_proj.scales', 'transformer.h.31.mlp.c_fc.g_idx', 'transformer.h.6.attn.c_attn.g_idx', 'transformer.h.13.mlp.c_proj.qzeros', 'transformer.h.34.mlp.c_proj.qweight', 'transformer.h.18.mlp.c_fc.scales', 'transformer.h.0.attn.c_proj.qweight', 'transformer.h.34.attn.c_proj.g_idx', 'transformer.h.31.attn.c_attn.qzeros', 'transformer.h.34.mlp.c_proj.scales', 'transformer.h.3.mlp.c_proj.qweight', 'transformer.h.19.attn.c_proj.scales', 'transformer.h.3.mlp.c_fc.qzeros', 'transformer.h.17.attn.c_attn.qzeros', 'transformer.h.30.mlp.c_proj.qweight', 'transformer.h.34.attn.c_attn.qweight', 'transformer.h.31.mlp.c_proj.scales', 'transformer.h.10.mlp.c_fc.qweight', 'transformer.h.0.attn.c_attn.g_idx', 'transformer.h.39.attn.c_proj.qzeros', 'transformer.h.27.attn.c_attn.scales', 'transformer.h.36.mlp.c_fc.g_idx', 'transformer.h.36.mlp.c_proj.scales', 'transformer.h.15.attn.c_attn.qweight', 'transformer.h.15.mlp.c_fc.qzeros', 'transformer.h.32.attn.c_proj.qweight', 'transformer.h.22.attn.c_proj.scales', 'transformer.h.2.mlp.c_fc.qweight', 'transformer.h.9.mlp.c_fc.qweight', 'transformer.h.2.attn.c_proj.scales', 'transformer.h.19.mlp.c_proj.qzeros', 'transformer.h.31.attn.c_proj.qweight', 'transformer.h.29.mlp.c_fc.qzeros', 'transformer.h.35.mlp.c_fc.scales', 'transformer.h.25.mlp.c_fc.scales', 'transformer.h.17.attn.c_attn.qweight', 'transformer.h.14.mlp.c_fc.qweight', 'transformer.h.27.mlp.c_fc.scales', 'transformer.h.15.attn.c_proj.g_idx', 'transformer.h.20.mlp.c_proj.qweight', 'transformer.h.16.mlp.c_proj.qzeros', 'transformer.h.5.attn.c_attn.qzeros', 'transformer.h.38.attn.c_attn.scales', 'transformer.h.21.mlp.c_fc.qzeros', 'transformer.h.15.attn.c_proj.scales', 'transformer.h.6.attn.c_attn.qweight', 'transformer.h.13.mlp.c_proj.scales', 'transformer.h.26.mlp.c_fc.qweight', 'transformer.h.25.mlp.c_proj.scales', 'transformer.h.37.attn.c_proj.qweight', 'transformer.h.33.mlp.c_proj.qzeros', 'transformer.h.35.attn.c_proj.scales', 'transformer.h.5.mlp.c_fc.scales', 'transformer.h.32.attn.c_attn.scales', 'transformer.h.5.mlp.c_fc.qzeros', 'transformer.h.5.mlp.c_proj.qweight', 'transformer.h.8.mlp.c_proj.qweight', 'transformer.h.3.mlp.c_proj.g_idx', 'transformer.h.27.attn.c_proj.scales', 'transformer.h.1.mlp.c_fc.qweight', 'transformer.h.17.mlp.c_proj.qweight', 'transformer.h.36.attn.c_attn.g_idx', 'transformer.h.7.mlp.c_proj.g_idx', 'transformer.h.33.attn.c_proj.qzeros', 'transformer.h.30.mlp.c_fc.qweight', 'transformer.h.10.attn.c_attn.qzeros', 'transformer.h.15.mlp.c_proj.qzeros', 'transformer.h.34.attn.c_attn.g_idx', 'transformer.h.5.attn.c_proj.g_idx', 'transformer.h.15.mlp.c_proj.g_idx', 'transformer.h.30.mlp.c_fc.g_idx', 'transformer.h.14.mlp.c_fc.scales', 'transformer.h.19.attn.c_proj.qzeros', 'transformer.h.11.mlp.c_fc.scales', 'transformer.h.23.mlp.c_proj.scales', 'transformer.h.11.mlp.c_fc.g_idx', 'transformer.h.5.mlp.c_proj.qzeros', 'transformer.h.17.mlp.c_fc.g_idx', 'transformer.h.23.mlp.c_fc.qweight', 'transformer.h.17.attn.c_attn.g_idx', 'transformer.h.32.mlp.c_fc.scales', 'transformer.h.8.attn.c_attn.scales', 'transformer.h.12.mlp.c_fc.qweight', 'transformer.h.18.attn.c_proj.scales', 'transformer.h.34.mlp.c_proj.g_idx', 'transformer.h.16.mlp.c_proj.scales', 'transformer.h.30.attn.c_proj.qzeros', 'transformer.h.19.mlp.c_fc.qzeros', 'transformer.h.3.attn.c_attn.qweight', 'transformer.h.5.attn.c_attn.qweight', 'transformer.h.0.attn.c_proj.qzeros', 'transformer.h.30.mlp.c_proj.g_idx', 'transformer.h.1.attn.c_proj.g_idx', 'transformer.h.21.attn.c_attn.scales', 'transformer.h.35.attn.c_attn.g_idx', 'transformer.h.29.mlp.c_proj.qzeros', 'transformer.h.19.attn.c_attn.g_idx', 'transformer.h.25.attn.c_proj.g_idx', 'transformer.h.39.attn.c_attn.qzeros', 'transformer.h.4.mlp.c_proj.scales', 'transformer.h.15.mlp.c_fc.g_idx', 'transformer.h.28.attn.c_proj.scales', 'transformer.h.29.mlp.c_fc.qweight', 'transformer.h.30.attn.c_attn.qzeros', 'transformer.h.5.attn.c_attn.scales', 'transformer.h.13.mlp.c_proj.g_idx', 'transformer.h.39.mlp.c_fc.qweight', 'transformer.h.22.attn.c_attn.qzeros', 'transformer.h.11.attn.c_proj.scales', 'transformer.h.9.attn.c_attn.qzeros', 'transformer.h.7.mlp.c_fc.g_idx', 'transformer.h.32.attn.c_proj.scales', 'transformer.h.8.attn.c_proj.qzeros', 'transformer.h.39.mlp.c_proj.scales', 'transformer.h.13.mlp.c_fc.qzeros']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = AutoModelForCausalLM.from_pretrained(BASE_MODEL, load_in_8bit=True, \n",
    "                                             use_safetensors=True, device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight 257392640\n",
      "transformer.wpe.weight 10485760\n",
      "transformer.h.0.ln_1.weight 5120\n",
      "transformer.h.0.ln_1.bias 5120\n",
      "transformer.h.0.attn.c_attn.weight 19660800\n",
      "transformer.h.0.attn.c_attn.bias 15360\n",
      "transformer.h.0.attn.c_proj.weight 6553600\n",
      "transformer.h.0.attn.c_proj.bias 5120\n",
      "transformer.h.0.ln_2.weight 5120\n",
      "transformer.h.0.ln_2.bias 5120\n",
      "transformer.h.0.mlp.c_fc.weight 26214400\n",
      "transformer.h.0.mlp.c_fc.bias 20480\n",
      "transformer.h.0.mlp.c_proj.weight 26214400\n",
      "transformer.h.0.mlp.c_proj.bias 5120\n",
      "transformer.h.1.ln_1.weight 5120\n",
      "transformer.h.1.ln_1.bias 5120\n",
      "transformer.h.1.attn.c_attn.weight 19660800\n",
      "transformer.h.1.attn.c_attn.bias 15360\n",
      "transformer.h.1.attn.c_proj.weight 6553600\n",
      "transformer.h.1.attn.c_proj.bias 5120\n",
      "transformer.h.1.ln_2.weight 5120\n",
      "transformer.h.1.ln_2.bias 5120\n",
      "transformer.h.1.mlp.c_fc.weight 26214400\n",
      "transformer.h.1.mlp.c_fc.bias 20480\n",
      "transformer.h.1.mlp.c_proj.weight 26214400\n",
      "transformer.h.1.mlp.c_proj.bias 5120\n",
      "transformer.h.2.ln_1.weight 5120\n",
      "transformer.h.2.ln_1.bias 5120\n",
      "transformer.h.2.attn.c_attn.weight 19660800\n",
      "transformer.h.2.attn.c_attn.bias 15360\n",
      "transformer.h.2.attn.c_proj.weight 6553600\n",
      "transformer.h.2.attn.c_proj.bias 5120\n",
      "transformer.h.2.ln_2.weight 5120\n",
      "transformer.h.2.ln_2.bias 5120\n",
      "transformer.h.2.mlp.c_fc.weight 26214400\n",
      "transformer.h.2.mlp.c_fc.bias 20480\n",
      "transformer.h.2.mlp.c_proj.weight 26214400\n",
      "transformer.h.2.mlp.c_proj.bias 5120\n",
      "transformer.h.3.ln_1.weight 5120\n",
      "transformer.h.3.ln_1.bias 5120\n",
      "transformer.h.3.attn.c_attn.weight 19660800\n",
      "transformer.h.3.attn.c_attn.bias 15360\n",
      "transformer.h.3.attn.c_proj.weight 6553600\n",
      "transformer.h.3.attn.c_proj.bias 5120\n",
      "transformer.h.3.ln_2.weight 5120\n",
      "transformer.h.3.ln_2.bias 5120\n",
      "transformer.h.3.mlp.c_fc.weight 26214400\n",
      "transformer.h.3.mlp.c_fc.bias 20480\n",
      "transformer.h.3.mlp.c_proj.weight 26214400\n",
      "transformer.h.3.mlp.c_proj.bias 5120\n",
      "transformer.h.4.ln_1.weight 5120\n",
      "transformer.h.4.ln_1.bias 5120\n",
      "transformer.h.4.attn.c_attn.weight 19660800\n",
      "transformer.h.4.attn.c_attn.bias 15360\n",
      "transformer.h.4.attn.c_proj.weight 6553600\n",
      "transformer.h.4.attn.c_proj.bias 5120\n",
      "transformer.h.4.ln_2.weight 5120\n",
      "transformer.h.4.ln_2.bias 5120\n",
      "transformer.h.4.mlp.c_fc.weight 26214400\n",
      "transformer.h.4.mlp.c_fc.bias 20480\n",
      "transformer.h.4.mlp.c_proj.weight 26214400\n",
      "transformer.h.4.mlp.c_proj.bias 5120\n",
      "transformer.h.5.ln_1.weight 5120\n",
      "transformer.h.5.ln_1.bias 5120\n",
      "transformer.h.5.attn.c_attn.weight 19660800\n",
      "transformer.h.5.attn.c_attn.bias 15360\n",
      "transformer.h.5.attn.c_proj.weight 6553600\n",
      "transformer.h.5.attn.c_proj.bias 5120\n",
      "transformer.h.5.ln_2.weight 5120\n",
      "transformer.h.5.ln_2.bias 5120\n",
      "transformer.h.5.mlp.c_fc.weight 26214400\n",
      "transformer.h.5.mlp.c_fc.bias 20480\n",
      "transformer.h.5.mlp.c_proj.weight 26214400\n",
      "transformer.h.5.mlp.c_proj.bias 5120\n",
      "transformer.h.6.ln_1.weight 5120\n",
      "transformer.h.6.ln_1.bias 5120\n",
      "transformer.h.6.attn.c_attn.weight 19660800\n",
      "transformer.h.6.attn.c_attn.bias 15360\n",
      "transformer.h.6.attn.c_proj.weight 6553600\n",
      "transformer.h.6.attn.c_proj.bias 5120\n",
      "transformer.h.6.ln_2.weight 5120\n",
      "transformer.h.6.ln_2.bias 5120\n",
      "transformer.h.6.mlp.c_fc.weight 26214400\n",
      "transformer.h.6.mlp.c_fc.bias 20480\n",
      "transformer.h.6.mlp.c_proj.weight 26214400\n",
      "transformer.h.6.mlp.c_proj.bias 5120\n",
      "transformer.h.7.ln_1.weight 5120\n",
      "transformer.h.7.ln_1.bias 5120\n",
      "transformer.h.7.attn.c_attn.weight 19660800\n",
      "transformer.h.7.attn.c_attn.bias 15360\n",
      "transformer.h.7.attn.c_proj.weight 6553600\n",
      "transformer.h.7.attn.c_proj.bias 5120\n",
      "transformer.h.7.ln_2.weight 5120\n",
      "transformer.h.7.ln_2.bias 5120\n",
      "transformer.h.7.mlp.c_fc.weight 26214400\n",
      "transformer.h.7.mlp.c_fc.bias 20480\n",
      "transformer.h.7.mlp.c_proj.weight 26214400\n",
      "transformer.h.7.mlp.c_proj.bias 5120\n",
      "transformer.h.8.ln_1.weight 5120\n",
      "transformer.h.8.ln_1.bias 5120\n",
      "transformer.h.8.attn.c_attn.weight 19660800\n",
      "transformer.h.8.attn.c_attn.bias 15360\n",
      "transformer.h.8.attn.c_proj.weight 6553600\n",
      "transformer.h.8.attn.c_proj.bias 5120\n",
      "transformer.h.8.ln_2.weight 5120\n",
      "transformer.h.8.ln_2.bias 5120\n",
      "transformer.h.8.mlp.c_fc.weight 26214400\n",
      "transformer.h.8.mlp.c_fc.bias 20480\n",
      "transformer.h.8.mlp.c_proj.weight 26214400\n",
      "transformer.h.8.mlp.c_proj.bias 5120\n",
      "transformer.h.9.ln_1.weight 5120\n",
      "transformer.h.9.ln_1.bias 5120\n",
      "transformer.h.9.attn.c_attn.weight 19660800\n",
      "transformer.h.9.attn.c_attn.bias 15360\n",
      "transformer.h.9.attn.c_proj.weight 6553600\n",
      "transformer.h.9.attn.c_proj.bias 5120\n",
      "transformer.h.9.ln_2.weight 5120\n",
      "transformer.h.9.ln_2.bias 5120\n",
      "transformer.h.9.mlp.c_fc.weight 26214400\n",
      "transformer.h.9.mlp.c_fc.bias 20480\n",
      "transformer.h.9.mlp.c_proj.weight 26214400\n",
      "transformer.h.9.mlp.c_proj.bias 5120\n",
      "transformer.h.10.ln_1.weight 5120\n",
      "transformer.h.10.ln_1.bias 5120\n",
      "transformer.h.10.attn.c_attn.weight 19660800\n",
      "transformer.h.10.attn.c_attn.bias 15360\n",
      "transformer.h.10.attn.c_proj.weight 6553600\n",
      "transformer.h.10.attn.c_proj.bias 5120\n",
      "transformer.h.10.ln_2.weight 5120\n",
      "transformer.h.10.ln_2.bias 5120\n",
      "transformer.h.10.mlp.c_fc.weight 26214400\n",
      "transformer.h.10.mlp.c_fc.bias 20480\n",
      "transformer.h.10.mlp.c_proj.weight 26214400\n",
      "transformer.h.10.mlp.c_proj.bias 5120\n",
      "transformer.h.11.ln_1.weight 5120\n",
      "transformer.h.11.ln_1.bias 5120\n",
      "transformer.h.11.attn.c_attn.weight 19660800\n",
      "transformer.h.11.attn.c_attn.bias 15360\n",
      "transformer.h.11.attn.c_proj.weight 6553600\n",
      "transformer.h.11.attn.c_proj.bias 5120\n",
      "transformer.h.11.ln_2.weight 5120\n",
      "transformer.h.11.ln_2.bias 5120\n",
      "transformer.h.11.mlp.c_fc.weight 26214400\n",
      "transformer.h.11.mlp.c_fc.bias 20480\n",
      "transformer.h.11.mlp.c_proj.weight 26214400\n",
      "transformer.h.11.mlp.c_proj.bias 5120\n",
      "transformer.h.12.ln_1.weight 5120\n",
      "transformer.h.12.ln_1.bias 5120\n",
      "transformer.h.12.attn.c_attn.weight 19660800\n",
      "transformer.h.12.attn.c_attn.bias 15360\n",
      "transformer.h.12.attn.c_proj.weight 6553600\n",
      "transformer.h.12.attn.c_proj.bias 5120\n",
      "transformer.h.12.ln_2.weight 5120\n",
      "transformer.h.12.ln_2.bias 5120\n",
      "transformer.h.12.mlp.c_fc.weight 26214400\n",
      "transformer.h.12.mlp.c_fc.bias 20480\n",
      "transformer.h.12.mlp.c_proj.weight 26214400\n",
      "transformer.h.12.mlp.c_proj.bias 5120\n",
      "transformer.h.13.ln_1.weight 5120\n",
      "transformer.h.13.ln_1.bias 5120\n",
      "transformer.h.13.attn.c_attn.weight 19660800\n",
      "transformer.h.13.attn.c_attn.bias 15360\n",
      "transformer.h.13.attn.c_proj.weight 6553600\n",
      "transformer.h.13.attn.c_proj.bias 5120\n",
      "transformer.h.13.ln_2.weight 5120\n",
      "transformer.h.13.ln_2.bias 5120\n",
      "transformer.h.13.mlp.c_fc.weight 26214400\n",
      "transformer.h.13.mlp.c_fc.bias 20480\n",
      "transformer.h.13.mlp.c_proj.weight 26214400\n",
      "transformer.h.13.mlp.c_proj.bias 5120\n",
      "transformer.h.14.ln_1.weight 5120\n",
      "transformer.h.14.ln_1.bias 5120\n",
      "transformer.h.14.attn.c_attn.weight 19660800\n",
      "transformer.h.14.attn.c_attn.bias 15360\n",
      "transformer.h.14.attn.c_proj.weight 6553600\n",
      "transformer.h.14.attn.c_proj.bias 5120\n",
      "transformer.h.14.ln_2.weight 5120\n",
      "transformer.h.14.ln_2.bias 5120\n",
      "transformer.h.14.mlp.c_fc.weight 26214400\n",
      "transformer.h.14.mlp.c_fc.bias 20480\n",
      "transformer.h.14.mlp.c_proj.weight 26214400\n",
      "transformer.h.14.mlp.c_proj.bias 5120\n",
      "transformer.h.15.ln_1.weight 5120\n",
      "transformer.h.15.ln_1.bias 5120\n",
      "transformer.h.15.attn.c_attn.weight 19660800\n",
      "transformer.h.15.attn.c_attn.bias 15360\n",
      "transformer.h.15.attn.c_proj.weight 6553600\n",
      "transformer.h.15.attn.c_proj.bias 5120\n",
      "transformer.h.15.ln_2.weight 5120\n",
      "transformer.h.15.ln_2.bias 5120\n",
      "transformer.h.15.mlp.c_fc.weight 26214400\n",
      "transformer.h.15.mlp.c_fc.bias 20480\n",
      "transformer.h.15.mlp.c_proj.weight 26214400\n",
      "transformer.h.15.mlp.c_proj.bias 5120\n",
      "transformer.h.16.ln_1.weight 5120\n",
      "transformer.h.16.ln_1.bias 5120\n",
      "transformer.h.16.attn.c_attn.weight 19660800\n",
      "transformer.h.16.attn.c_attn.bias 15360\n",
      "transformer.h.16.attn.c_proj.weight 6553600\n",
      "transformer.h.16.attn.c_proj.bias 5120\n",
      "transformer.h.16.ln_2.weight 5120\n",
      "transformer.h.16.ln_2.bias 5120\n",
      "transformer.h.16.mlp.c_fc.weight 26214400\n",
      "transformer.h.16.mlp.c_fc.bias 20480\n",
      "transformer.h.16.mlp.c_proj.weight 26214400\n",
      "transformer.h.16.mlp.c_proj.bias 5120\n",
      "transformer.h.17.ln_1.weight 5120\n",
      "transformer.h.17.ln_1.bias 5120\n",
      "transformer.h.17.attn.c_attn.weight 19660800\n",
      "transformer.h.17.attn.c_attn.bias 15360\n",
      "transformer.h.17.attn.c_proj.weight 6553600\n",
      "transformer.h.17.attn.c_proj.bias 5120\n",
      "transformer.h.17.ln_2.weight 5120\n",
      "transformer.h.17.ln_2.bias 5120\n",
      "transformer.h.17.mlp.c_fc.weight 26214400\n",
      "transformer.h.17.mlp.c_fc.bias 20480\n",
      "transformer.h.17.mlp.c_proj.weight 26214400\n",
      "transformer.h.17.mlp.c_proj.bias 5120\n",
      "transformer.h.18.ln_1.weight 5120\n",
      "transformer.h.18.ln_1.bias 5120\n",
      "transformer.h.18.attn.c_attn.weight 19660800\n",
      "transformer.h.18.attn.c_attn.bias 15360\n",
      "transformer.h.18.attn.c_proj.weight 6553600\n",
      "transformer.h.18.attn.c_proj.bias 5120\n",
      "transformer.h.18.ln_2.weight 5120\n",
      "transformer.h.18.ln_2.bias 5120\n",
      "transformer.h.18.mlp.c_fc.weight 26214400\n",
      "transformer.h.18.mlp.c_fc.bias 20480\n",
      "transformer.h.18.mlp.c_proj.weight 26214400\n",
      "transformer.h.18.mlp.c_proj.bias 5120\n",
      "transformer.h.19.ln_1.weight 5120\n",
      "transformer.h.19.ln_1.bias 5120\n",
      "transformer.h.19.attn.c_attn.weight 19660800\n",
      "transformer.h.19.attn.c_attn.bias 15360\n",
      "transformer.h.19.attn.c_proj.weight 6553600\n",
      "transformer.h.19.attn.c_proj.bias 5120\n",
      "transformer.h.19.ln_2.weight 5120\n",
      "transformer.h.19.ln_2.bias 5120\n",
      "transformer.h.19.mlp.c_fc.weight 26214400\n",
      "transformer.h.19.mlp.c_fc.bias 20480\n",
      "transformer.h.19.mlp.c_proj.weight 26214400\n",
      "transformer.h.19.mlp.c_proj.bias 5120\n",
      "transformer.h.20.ln_1.weight 5120\n",
      "transformer.h.20.ln_1.bias 5120\n",
      "transformer.h.20.attn.c_attn.weight 19660800\n",
      "transformer.h.20.attn.c_attn.bias 15360\n",
      "transformer.h.20.attn.c_proj.weight 6553600\n",
      "transformer.h.20.attn.c_proj.bias 5120\n",
      "transformer.h.20.ln_2.weight 5120\n",
      "transformer.h.20.ln_2.bias 5120\n",
      "transformer.h.20.mlp.c_fc.weight 26214400\n",
      "transformer.h.20.mlp.c_fc.bias 20480\n",
      "transformer.h.20.mlp.c_proj.weight 26214400\n",
      "transformer.h.20.mlp.c_proj.bias 5120\n",
      "transformer.h.21.ln_1.weight 5120\n",
      "transformer.h.21.ln_1.bias 5120\n",
      "transformer.h.21.attn.c_attn.weight 19660800\n",
      "transformer.h.21.attn.c_attn.bias 15360\n",
      "transformer.h.21.attn.c_proj.weight 6553600\n",
      "transformer.h.21.attn.c_proj.bias 5120\n",
      "transformer.h.21.ln_2.weight 5120\n",
      "transformer.h.21.ln_2.bias 5120\n",
      "transformer.h.21.mlp.c_fc.weight 26214400\n",
      "transformer.h.21.mlp.c_fc.bias 20480\n",
      "transformer.h.21.mlp.c_proj.weight 26214400\n",
      "transformer.h.21.mlp.c_proj.bias 5120\n",
      "transformer.h.22.ln_1.weight 5120\n",
      "transformer.h.22.ln_1.bias 5120\n",
      "transformer.h.22.attn.c_attn.weight 19660800\n",
      "transformer.h.22.attn.c_attn.bias 15360\n",
      "transformer.h.22.attn.c_proj.weight 6553600\n",
      "transformer.h.22.attn.c_proj.bias 5120\n",
      "transformer.h.22.ln_2.weight 5120\n",
      "transformer.h.22.ln_2.bias 5120\n",
      "transformer.h.22.mlp.c_fc.weight 26214400\n",
      "transformer.h.22.mlp.c_fc.bias 20480\n",
      "transformer.h.22.mlp.c_proj.weight 26214400\n",
      "transformer.h.22.mlp.c_proj.bias 5120\n",
      "transformer.h.23.ln_1.weight 5120\n",
      "transformer.h.23.ln_1.bias 5120\n",
      "transformer.h.23.attn.c_attn.weight 19660800\n",
      "transformer.h.23.attn.c_attn.bias 15360\n",
      "transformer.h.23.attn.c_proj.weight 6553600\n",
      "transformer.h.23.attn.c_proj.bias 5120\n",
      "transformer.h.23.ln_2.weight 5120\n",
      "transformer.h.23.ln_2.bias 5120\n",
      "transformer.h.23.mlp.c_fc.weight 26214400\n",
      "transformer.h.23.mlp.c_fc.bias 20480\n",
      "transformer.h.23.mlp.c_proj.weight 26214400\n",
      "transformer.h.23.mlp.c_proj.bias 5120\n",
      "transformer.h.24.ln_1.weight 5120\n",
      "transformer.h.24.ln_1.bias 5120\n",
      "transformer.h.24.attn.c_attn.weight 19660800\n",
      "transformer.h.24.attn.c_attn.bias 15360\n",
      "transformer.h.24.attn.c_proj.weight 6553600\n",
      "transformer.h.24.attn.c_proj.bias 5120\n",
      "transformer.h.24.ln_2.weight 5120\n",
      "transformer.h.24.ln_2.bias 5120\n",
      "transformer.h.24.mlp.c_fc.weight 26214400\n",
      "transformer.h.24.mlp.c_fc.bias 20480\n",
      "transformer.h.24.mlp.c_proj.weight 26214400\n",
      "transformer.h.24.mlp.c_proj.bias 5120\n",
      "transformer.h.25.ln_1.weight 5120\n",
      "transformer.h.25.ln_1.bias 5120\n",
      "transformer.h.25.attn.c_attn.weight 19660800\n",
      "transformer.h.25.attn.c_attn.bias 15360\n",
      "transformer.h.25.attn.c_proj.weight 6553600\n",
      "transformer.h.25.attn.c_proj.bias 5120\n",
      "transformer.h.25.ln_2.weight 5120\n",
      "transformer.h.25.ln_2.bias 5120\n",
      "transformer.h.25.mlp.c_fc.weight 26214400\n",
      "transformer.h.25.mlp.c_fc.bias 20480\n",
      "transformer.h.25.mlp.c_proj.weight 26214400\n",
      "transformer.h.25.mlp.c_proj.bias 5120\n",
      "transformer.h.26.ln_1.weight 5120\n",
      "transformer.h.26.ln_1.bias 5120\n",
      "transformer.h.26.attn.c_attn.weight 19660800\n",
      "transformer.h.26.attn.c_attn.bias 15360\n",
      "transformer.h.26.attn.c_proj.weight 6553600\n",
      "transformer.h.26.attn.c_proj.bias 5120\n",
      "transformer.h.26.ln_2.weight 5120\n",
      "transformer.h.26.ln_2.bias 5120\n",
      "transformer.h.26.mlp.c_fc.weight 26214400\n",
      "transformer.h.26.mlp.c_fc.bias 20480\n",
      "transformer.h.26.mlp.c_proj.weight 26214400\n",
      "transformer.h.26.mlp.c_proj.bias 5120\n",
      "transformer.h.27.ln_1.weight 5120\n",
      "transformer.h.27.ln_1.bias 5120\n",
      "transformer.h.27.attn.c_attn.weight 19660800\n",
      "transformer.h.27.attn.c_attn.bias 15360\n",
      "transformer.h.27.attn.c_proj.weight 6553600\n",
      "transformer.h.27.attn.c_proj.bias 5120\n",
      "transformer.h.27.ln_2.weight 5120\n",
      "transformer.h.27.ln_2.bias 5120\n",
      "transformer.h.27.mlp.c_fc.weight 26214400\n",
      "transformer.h.27.mlp.c_fc.bias 20480\n",
      "transformer.h.27.mlp.c_proj.weight 26214400\n",
      "transformer.h.27.mlp.c_proj.bias 5120\n",
      "transformer.h.28.ln_1.weight 5120\n",
      "transformer.h.28.ln_1.bias 5120\n",
      "transformer.h.28.attn.c_attn.weight 19660800\n",
      "transformer.h.28.attn.c_attn.bias 15360\n",
      "transformer.h.28.attn.c_proj.weight 6553600\n",
      "transformer.h.28.attn.c_proj.bias 5120\n",
      "transformer.h.28.ln_2.weight 5120\n",
      "transformer.h.28.ln_2.bias 5120\n",
      "transformer.h.28.mlp.c_fc.weight 26214400\n",
      "transformer.h.28.mlp.c_fc.bias 20480\n",
      "transformer.h.28.mlp.c_proj.weight 26214400\n",
      "transformer.h.28.mlp.c_proj.bias 5120\n",
      "transformer.h.29.ln_1.weight 5120\n",
      "transformer.h.29.ln_1.bias 5120\n",
      "transformer.h.29.attn.c_attn.weight 19660800\n",
      "transformer.h.29.attn.c_attn.bias 15360\n",
      "transformer.h.29.attn.c_proj.weight 6553600\n",
      "transformer.h.29.attn.c_proj.bias 5120\n",
      "transformer.h.29.ln_2.weight 5120\n",
      "transformer.h.29.ln_2.bias 5120\n",
      "transformer.h.29.mlp.c_fc.weight 26214400\n",
      "transformer.h.29.mlp.c_fc.bias 20480\n",
      "transformer.h.29.mlp.c_proj.weight 26214400\n",
      "transformer.h.29.mlp.c_proj.bias 5120\n",
      "transformer.h.30.ln_1.weight 5120\n",
      "transformer.h.30.ln_1.bias 5120\n",
      "transformer.h.30.attn.c_attn.weight 19660800\n",
      "transformer.h.30.attn.c_attn.bias 15360\n",
      "transformer.h.30.attn.c_proj.weight 6553600\n",
      "transformer.h.30.attn.c_proj.bias 5120\n",
      "transformer.h.30.ln_2.weight 5120\n",
      "transformer.h.30.ln_2.bias 5120\n",
      "transformer.h.30.mlp.c_fc.weight 26214400\n",
      "transformer.h.30.mlp.c_fc.bias 20480\n",
      "transformer.h.30.mlp.c_proj.weight 26214400\n",
      "transformer.h.30.mlp.c_proj.bias 5120\n",
      "transformer.h.31.ln_1.weight 5120\n",
      "transformer.h.31.ln_1.bias 5120\n",
      "transformer.h.31.attn.c_attn.weight 19660800\n",
      "transformer.h.31.attn.c_attn.bias 15360\n",
      "transformer.h.31.attn.c_proj.weight 6553600\n",
      "transformer.h.31.attn.c_proj.bias 5120\n",
      "transformer.h.31.ln_2.weight 5120\n",
      "transformer.h.31.ln_2.bias 5120\n",
      "transformer.h.31.mlp.c_fc.weight 26214400\n",
      "transformer.h.31.mlp.c_fc.bias 20480\n",
      "transformer.h.31.mlp.c_proj.weight 26214400\n",
      "transformer.h.31.mlp.c_proj.bias 5120\n",
      "transformer.h.32.ln_1.weight 5120\n",
      "transformer.h.32.ln_1.bias 5120\n",
      "transformer.h.32.attn.c_attn.weight 19660800\n",
      "transformer.h.32.attn.c_attn.bias 15360\n",
      "transformer.h.32.attn.c_proj.weight 6553600\n",
      "transformer.h.32.attn.c_proj.bias 5120\n",
      "transformer.h.32.ln_2.weight 5120\n",
      "transformer.h.32.ln_2.bias 5120\n",
      "transformer.h.32.mlp.c_fc.weight 26214400\n",
      "transformer.h.32.mlp.c_fc.bias 20480\n",
      "transformer.h.32.mlp.c_proj.weight 26214400\n",
      "transformer.h.32.mlp.c_proj.bias 5120\n",
      "transformer.h.33.ln_1.weight 5120\n",
      "transformer.h.33.ln_1.bias 5120\n",
      "transformer.h.33.attn.c_attn.weight 19660800\n",
      "transformer.h.33.attn.c_attn.bias 15360\n",
      "transformer.h.33.attn.c_proj.weight 6553600\n",
      "transformer.h.33.attn.c_proj.bias 5120\n",
      "transformer.h.33.ln_2.weight 5120\n",
      "transformer.h.33.ln_2.bias 5120\n",
      "transformer.h.33.mlp.c_fc.weight 26214400\n",
      "transformer.h.33.mlp.c_fc.bias 20480\n",
      "transformer.h.33.mlp.c_proj.weight 26214400\n",
      "transformer.h.33.mlp.c_proj.bias 5120\n",
      "transformer.h.34.ln_1.weight 5120\n",
      "transformer.h.34.ln_1.bias 5120\n",
      "transformer.h.34.attn.c_attn.weight 19660800\n",
      "transformer.h.34.attn.c_attn.bias 15360\n",
      "transformer.h.34.attn.c_proj.weight 6553600\n",
      "transformer.h.34.attn.c_proj.bias 5120\n",
      "transformer.h.34.ln_2.weight 5120\n",
      "transformer.h.34.ln_2.bias 5120\n",
      "transformer.h.34.mlp.c_fc.weight 26214400\n",
      "transformer.h.34.mlp.c_fc.bias 20480\n",
      "transformer.h.34.mlp.c_proj.weight 26214400\n",
      "transformer.h.34.mlp.c_proj.bias 5120\n",
      "transformer.h.35.ln_1.weight 5120\n",
      "transformer.h.35.ln_1.bias 5120\n",
      "transformer.h.35.attn.c_attn.weight 19660800\n",
      "transformer.h.35.attn.c_attn.bias 15360\n",
      "transformer.h.35.attn.c_proj.weight 6553600\n",
      "transformer.h.35.attn.c_proj.bias 5120\n",
      "transformer.h.35.ln_2.weight 5120\n",
      "transformer.h.35.ln_2.bias 5120\n",
      "transformer.h.35.mlp.c_fc.weight 26214400\n",
      "transformer.h.35.mlp.c_fc.bias 20480\n",
      "transformer.h.35.mlp.c_proj.weight 26214400\n",
      "transformer.h.35.mlp.c_proj.bias 5120\n",
      "transformer.h.36.ln_1.weight 5120\n",
      "transformer.h.36.ln_1.bias 5120\n",
      "transformer.h.36.attn.c_attn.weight 19660800\n",
      "transformer.h.36.attn.c_attn.bias 15360\n",
      "transformer.h.36.attn.c_proj.weight 6553600\n",
      "transformer.h.36.attn.c_proj.bias 5120\n",
      "transformer.h.36.ln_2.weight 5120\n",
      "transformer.h.36.ln_2.bias 5120\n",
      "transformer.h.36.mlp.c_fc.weight 26214400\n",
      "transformer.h.36.mlp.c_fc.bias 20480\n",
      "transformer.h.36.mlp.c_proj.weight 26214400\n",
      "transformer.h.36.mlp.c_proj.bias 5120\n",
      "transformer.h.37.ln_1.weight 5120\n",
      "transformer.h.37.ln_1.bias 5120\n",
      "transformer.h.37.attn.c_attn.weight 19660800\n",
      "transformer.h.37.attn.c_attn.bias 15360\n",
      "transformer.h.37.attn.c_proj.weight 6553600\n",
      "transformer.h.37.attn.c_proj.bias 5120\n",
      "transformer.h.37.ln_2.weight 5120\n",
      "transformer.h.37.ln_2.bias 5120\n",
      "transformer.h.37.mlp.c_fc.weight 26214400\n",
      "transformer.h.37.mlp.c_fc.bias 20480\n",
      "transformer.h.37.mlp.c_proj.weight 26214400\n",
      "transformer.h.37.mlp.c_proj.bias 5120\n",
      "transformer.h.38.ln_1.weight 5120\n",
      "transformer.h.38.ln_1.bias 5120\n",
      "transformer.h.38.attn.c_attn.weight 19660800\n",
      "transformer.h.38.attn.c_attn.bias 15360\n",
      "transformer.h.38.attn.c_proj.weight 6553600\n",
      "transformer.h.38.attn.c_proj.bias 5120\n",
      "transformer.h.38.ln_2.weight 5120\n",
      "transformer.h.38.ln_2.bias 5120\n",
      "transformer.h.38.mlp.c_fc.weight 26214400\n",
      "transformer.h.38.mlp.c_fc.bias 20480\n",
      "transformer.h.38.mlp.c_proj.weight 26214400\n",
      "transformer.h.38.mlp.c_proj.bias 5120\n",
      "transformer.h.39.ln_1.weight 5120\n",
      "transformer.h.39.ln_1.bias 5120\n",
      "transformer.h.39.attn.c_attn.weight 19660800\n",
      "transformer.h.39.attn.c_attn.bias 15360\n",
      "transformer.h.39.attn.c_proj.weight 6553600\n",
      "transformer.h.39.attn.c_proj.bias 5120\n",
      "transformer.h.39.ln_2.weight 5120\n",
      "transformer.h.39.ln_2.bias 5120\n",
      "transformer.h.39.mlp.c_fc.weight 26214400\n",
      "transformer.h.39.mlp.c_fc.bias 20480\n",
      "transformer.h.39.mlp.c_proj.weight 26214400\n",
      "transformer.h.39.mlp.c_proj.bias 5120\n",
      "transformer.ln_f.weight 5120\n",
      "transformer.ln_f.bias 5120\n"
     ]
    }
   ],
   "source": [
    "for name, param in MODEL.named_parameters():\n",
    "    print(name, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = IA3Config(\n",
    "        peft_type=\"IA3\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"c_attn\", \"c_proj\",\"c_fc\"],\n",
    "        feedforward_modules=['c_fc'])\n",
    "\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(BASE_MODEL, #load_in_8bit=True, \n",
    "                                             use_safetensors=True, device_map='cpu')\n",
    "\n",
    "# frozing full base model\n",
    "for param in MODEL.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# wraps model to ia3\n",
    "IA3_MODEL = get_peft_model(MODEL, CONFIG)\n",
    "\n",
    "for name, param in IA3_MODEL.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "\n",
    "# checking trainable parameters\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(IA3_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(np.random.randint(30, 50, size=(1,94)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[49., 49., 32., 32., 35., 40., 40., 36., 47., 35., 43., 42., 39., 46.,\n",
       "         37., 32., 37., 32., 36., 32., 42., 33., 33., 45., 41., 41., 33., 30.,\n",
       "         43., 48., 40., 32., 32., 39., 31., 32., 41., 49., 49., 32., 45., 47.,\n",
       "         34., 40., 39., 45., 49., 31., 40., 32., 38., 40., 44., 37., 46., 32.,\n",
       "         39., 47., 32., 41., 47., 32., 31., 45., 39., 39., 30., 37., 35., 35.,\n",
       "         45., 30., 47., 32., 44., 46., 31., 41., 38., 36., 33., 38., 48., 40.,\n",
       "         43., 39., 34., 41., 36., 39., 48., 48., 36., 35.]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m MODEL(input_ids\u001b[39m=\u001b[39;49minput_ids\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1075\u001b[0m     input_ids,\n\u001b[1;32m   1076\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1077\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1078\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1079\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1080\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1081\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1082\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1083\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1084\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1085\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1086\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1087\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:837\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    834\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_layer)\n\u001b[1;32m    836\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwte(input_ids)\n\u001b[1;32m    838\u001b[0m position_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwpe(position_ids)\n\u001b[1;32m    839\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m position_embeds\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "MODEL(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = torch.optim.Adam(IA3_MODEL.parameters(), \n",
    "                             lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model, loader, optimizer):\n",
    "    \"\"\"Standard PyTorch training, one epoch\"\"\"\n",
    "    model.train()\n",
    "    acc_loss, counter = [], 1\n",
    "    process = tqdm.tqdm(loader)\n",
    "    for batch in process:\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels'])\n",
    "        \n",
    "        loss = out['loss']\n",
    "        acc_loss += loss.item()\n",
    "        show_dict = {'Train cur mean Loss': f'{acc_loss/counter:.6f}'}\n",
    "        process.set_postfix(show_dict)\n",
    "\n",
    "    return round(acc_loss/counter, 5)\n",
    "\n",
    "def eval_one(model, loader):\n",
    "    \"\"\"Standard PyTorch evaluation, one epoch\"\"\"\n",
    "    model.eval()\n",
    "    acc_loss, counter = [], 1\n",
    "    process = tqdm.tqdm(loader)\n",
    "    for batch in process:\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'])\n",
    "            \n",
    "        loss = out['loss']\n",
    "        acc_loss += loss.item()\n",
    "        show_dict = {'Val cur mean Loss': f'{acc_loss/counter:.6f}'}\n",
    "        process.set_postfix(show_dict)\n",
    "\n",
    "    return round(acc_loss/counter, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemsDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer):\n",
    "\n",
    "        self.df = pd.read_csv(file, sep=';') \n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_col = 'text'\n",
    "        self.title_col = 'title'\n",
    "        self.end_of_text_token = '<|endoftext|>'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def formate_sample(self, index):\n",
    "        text, title = self.df[self.text_col][index], self.df[self.title_col][index] \n",
    "        \n",
    "        formated_sttring = f': \"{title}\". : {text} {self.end_of_text_token}'\n",
    "        \n",
    "        encoded_string = self.tokenizer(formated_sttring, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoded_string['input_ids'],\n",
    "            'attention_mask': encoded_string['atttention_mask'],\n",
    "            'labels': encoded_string['input_ids']\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.formate_sample(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PoemsDataset(TRAIN_FILE,TOKENIZER)\n",
    "train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=1)\n",
    "\n",
    "eval_ds = PoemsDataset(EVAL_FILE,TOKENIZER)\n",
    "eval_dataloader = DataLoader(eval_ds, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eval_loss = None\n",
    "for i_epoch in range(EPOCHS):\n",
    "     loss_train = train_one(IA3_MODEL, train_dataloader, OPTIMIZER)\n",
    "     loss_val = eval_one(IA3_MODEL, eval_dataloader)\n",
    "     \n",
    "     #\n",
    "     print(f'{i_epoch} : loss_train={loss_train}, loss_val={loss_val}')\n",
    "     metrics = {\n",
    "          \"epoch\": i_epoch,\n",
    "          \"train_loss\": loss_train, \n",
    "          \"eval_loss\": loss_val\n",
    "          }\n",
    "     \n",
    "     # \n",
    "     if best_eval_loss is None or best_eval_loss >= loss_val:\n",
    "          print(\"Saving new best AI3 adapter!\")\n",
    "          IA3_MODEL.save_pretrained(ADAPTER_DIR)\n",
    "          best_eval_loss = loss_val\n",
    "     elif i_epoch == (EPOCHS-1):\n",
    "          print(\"Saving last AI3 adapter!\")\n",
    "          IA3_MODEL.save_pretrained(ADAPTER_DIR + '(last_train)')\n",
    "\n",
    "     #\n",
    "     m_object = json.dumps(metrics, indent=2, ensure_ascii=False)\n",
    "     with open(LOG_DIR + f'epoch_{i_epoch}', 'w',encoding='utf-8') as fd:\n",
    "          fd.write(m_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
