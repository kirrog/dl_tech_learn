BATCH_SIZE:  1
LR:  0.0003
EPOCHS:  12
DEVICE:  cuda:0
Load tokenizer
Init config
Load base model
Wraps model to ia3
PeftModelForCausalLM(
  (base_model): IA3Model(
    (model): GPT2LMHeadModel(
      (transformer): GPT2Model(
        (wte): Embedding(50272, 5120)
        (wpe): Embedding(2048, 5120)
        (drop): Dropout(p=0.1, inplace=False)
        (h): ModuleList(
          (0): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (12): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (13): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (14): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (15): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (16): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (17): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (18): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (19): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (20): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (21): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (22): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (23): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (24): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (25): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (26): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (27): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (28): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (29): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (30): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (31): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (32): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (33): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (34): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (35): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (36): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (37): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (38): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (39): GPT2Block(
            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (attn): GPT2Attention(
              (c_attn): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=15360, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 15360x1 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (attn_dropout): Dropout(p=0.1, inplace=False)
              (resid_dropout): Dropout(p=0.1, inplace=False)
            )
            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
            (mlp): GPT2MLP(
              (c_fc): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=5120, out_features=20480, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 1x5120 (GPU 0)])
              )
              (c_proj): ia3.Linear8bitLt(
                (base_layer): Linear8bitLt(in_features=20480, out_features=5120, bias=True)
                (ia3_l): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 5120x1 (GPU 0)])
              )
              (act): NewGELUActivation()
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
        (ln_f): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)
      )
      (lm_head): Linear(in_features=5120, out_features=50272, bias=False)
    )
  )
)
trainable params: 1228800 || all params: 3417507840 || trainable%: 0.03595602578047048
